[
  {
    "objectID": "Aula1.html",
    "href": "Aula1.html",
    "title": "Introdução ao R",
    "section": "",
    "text": "Ao adentrar o fascinante mundo da análise de dados com o Rstudio, é importante saber por onde começar. Existem diversos materiais interessantes e extremamente utéis disponíveis, basta acessá-los e saber usar as informações a seu favor. Dentre os materias disponíveis, livros como Introdução ciência de dados no R, Introduction to R e The Art of Data Science são essenciais para obter uma compreensão básica da ciência de dados e um entendimento mais aprofundada de como realizar exploração e nálise de dados corretamente no R.\nÉ importante ressaltar que os materiais liberados neste site são aulas que foram disponibilizadas com o intuíto de agregar ao conhecimento mas, como se trata de assuntos complexos, não foi possível realizar anotações de qualidade sobre todos os assuntos abordados, portanto, esse não deve ser o seu ponto de partida para estudo das anpalises. Estes materiais, por si só, não são capazes de elucidar questões complexas do R, por isso é importante não focar apenas neste conteúdo. Assim, damos início a nossa jornada de conhecimento no R apresentando alguns de seus mais básicos objetos e funções.\n\n\nObjeto: um objeto é simplesmente um nome que guarda um valor e, para criá-lo, utilizamos o operador <-. Ex.: a <- 1. No R, uma base de dados é representada por objetos chamados de data frames. Sempre que roda-se uma função, o código que ela guarda será executado e um resultado nos será devolvido. Entre parênteses, após o nome da função, temos o que chamamos de argumentos. Uma função pode ter qualquer número de argumentos e eles são sempre separados por vírgula. Ex.: sum(1, 2). A função sum() recebeu os argumentos 1 e 2.\nClasses: para criar texto no R, colocamos os caracteres entre aspas (” “). As aspas servem para diferenciar nomes (objetos, funções, pacotes) de textos (letras e palavras). Os textos são muito comuns em variáveis categóricas.As classes mais básicas dentro do R são: numeric, character e logical.\nVetores: Vetores no R são apenas conjuntos indexados de valores. Para criá-los, basta colocar os valores separados por vírgulas dentro de um c().Cada coluna de um data frame será representada como um vetor. Ex.: vetor1 <- c(1, 5, 3, -10).Uma maneira fácil de criar um vetor com uma sequência de números é utilizar o operador :.\nTestes lógicos: uma operação lógica nada mais é do que um teste que retorna verdadeiro ou falso. O verdadeiro no R vai ser representado pelo valor TRUE e o falso pelo valor FALSE. Esses nomes no R são reservados, isto é, você não pode chamar nenhum objeto de TRUE ou FALSE. Ex.: para testar se um valor é igual ao outro (operador ==) - 1 == 1 [TRUE] ou 1 == 2 [FALSE]. Alguns dos principais operadores lógicos são:\nx < y (x menor que y?) x <= y (x menor ou igual a y?) x > y x (maior que y?) x >= y (x maior ou igual a y?) x == y (x igual a y?) x != y (x diferente de y?) !x (Negativa de x) x | y (x ou y são verdadeiros?) x & y (x e y são verdadeiros?) x %in% y (x pertence a y?) xor(x, y) x ou y são verdadeiros (apenas um deles)?\nValores especiais: NA - representa a ausência de informação, isto é, a informação existe, mas nós (e o R) não sabemos qual é. O NA para o R nada mais é do que o valor faltante ou omisso da Estatística.\nOperador pipe: a ideia do operador %>% (pipe) é bem simples: usar o valor resultante da expressão do lado esquerdo como primeiro argumento da função do lado direito. Usa-se o pipe (|> ou %>%) para enfatizar uma sequência de comandos ou ações no chunk e para evitar adicionar o nome do data frame dentro da função ggplot. O pipe deve ter sempre um espaço antes dele e, geralmente, deve ser seguido por uma nova linha. Após a primeira etapa, cada linha deve ter dois espaços, o que torna mais fácil adicionar outras etapas ou reorganizar as já existentes.\n**Operador cifrão: usar o comando cifrão permite acessar colunas pelo nome. O uso é basicamente o seguinte - dados(cifrão)variável, onde - dados especifica o conjunto de dados e variável a variável que deseja extrair. Por exemplo, para extrair os dados de macacos use: > macac$macacos."
  },
  {
    "objectID": "Aula10.html",
    "href": "Aula10.html",
    "title": "ANOVA",
    "section": "",
    "text": "A ANOVA (Análise de Variância) é um método estatístico que permite comparar as médias de três ou mais grupos e verificar se há diferenças significativas entre elas. A ANOVA usa o teste F para testar a hipótese nula de que as médias populacionais são iguais contra a hipótese alternativa de que pelo menos uma média é diferente das demais.\n\n\nComo dito, a ANOVA é um método utilizado para comparar as médias de três ou mais grupos independentes. Então, dada a situação: Experimento com um fator e em delineamento inteiramente casualizado para comparar o crescimento micelial de diferentes espécies de um fungo fitopatogênico. A resposta a ser estudada é a TCM = taxa de crescimento micelial. Pergunta a ser resposndida: Há efeito da espécie no crescimento micelial?\nPreparo pré-análise: carregamento de pacotes e importação dos dados.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nmicelial <- read_excel(\"dados-diversos.xlsx\", \"micelial\")\nhead(micelial)\n\n# A tibble: 6 × 3\n  especie   rep   tcm\n  <chr>   <dbl> <dbl>\n1 Fasi        1  1.5 \n2 Fasi        2  1.59\n3 Fasi        3  1.52\n4 Fasi        4  1.52\n5 Fasi        5  1.24\n6 Fasi        6  1.29\n\n\nVisualização dos dados:\n\nmicelial |>\n  ggplot(aes(especie, tcm))+\n  geom_boxplot()\n\n\n\n\n\n\nPara verificar os dados usando anova, um novo modelo para atribuir a função aov() contendo os argumentos tratamento em função da variável resposta deve ser criado (ex.: tcm ~ espécie), o banco de dados referido deve ser enunciado após o argumento separado por vírgula seguido do nome data = nome do conjunto de dados (ex.: micelial). Depois disso, pede um quadro de resumo do novo modelo criado.\n\naov1 <- aov(tcm ~ especie, data = micelial)\nsummary(aov1)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nespecie      4 0.4692 0.11729   1.983  0.117\nResiduals   37 2.1885 0.05915               \n\n\nInterpretação: Nesse conjunto de dados, não há diferença na media micelial (não há efeito significativo da espécie sobre o cresc. micelial).\n\n\n\nDepois de fazer a anova, testa-se as premissas. É mais importante os dados serem homogêneos do que normais. Para testar as premissas, é necessário instalar e carregar o pacote performance e o pacote DHARMa. O pacote performance permite checar as premissas (check_), já o pacote DHARMA ((Distributed Hierarchical Accumulation of Residuals for Generalized Linear Models in R) é para visualizar os dados pelo diagnóstico do resíduo. O pacote DHARMa permite faz uma comparação dos resíduos simulados, que são gerados pelo pacote, com os resíduos observados e ver graficamente quando a distribuição dos dados não é normal e/ou quando há variação heterocedástica. Após isso, deve-se fazer o teste de normalidade dos resíduos com a interação entre a anova e os resíduos.\n\nlibrary(performance)\ncheck_heteroscedasticity(aov1)\n\nOK: Error variance appears to be homoscedastic (p = 0.175).\n\ncheck_normality(aov1)\n\nOK: residuals appear as normally distributed (p = 0.074).\n\nlibrary(DHARMa)\nhist (aov1$residuals)\n\n\n\nqqnorm(aov1$residuals)\nqqline(aov1$residuals)\n\n\n\nplot(simulateResiduals(aov1))\n\n\n\nshapiro.test(aov1$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aov1$residuals\nW = 0.95101, p-value = 0.07022\n\n\nInterepretação:Premissas da anova atendidas. Efeito não significativo de espécies.\n\n\n\n\nCaso nos deparemos com o não atendimento das premissas, o que devemos fazer?\nSituação: conjunto InsectSprays: efeito de inseticida na mortalidade de insetos (Beall, 1942).Dados no pacote “datasets” do R.\n\ninsects <- tibble::as_tibble(InsectSprays) |>\n  dplyr::select(spray, count)\ninsects\n\n# A tibble: 72 × 2\n   spray count\n   <fct> <dbl>\n 1 A        10\n 2 A         7\n 3 A        20\n 4 A        14\n 5 A        14\n 6 A        12\n 7 A        10\n 8 A        23\n 9 A        17\n10 A        20\n# ℹ 62 more rows\n\n\nAnálise visual dos dados:\n\ninsects |>\n  ggplot(aes(spray, count))+\n  geom_boxplot()\n\n\n\n\nRodando o modelo de anova: Aparentemente - pelo visual do gráfico - os dadis apresentaram-se não paramétricos. Quando se analisa um conjunto de dados e esses dados apresentam-se não paramétricos, a forma de trabalhar esses dados torna-se um pouco diferente. Antes de partir para testes diferentes, deve-se comprovar, por meio da anova e das premissas, que os dados realmente não são normais e homogêneos.\n\naov2 <- aov(count ~ spray, data = insects)\nsummary(aov2)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \nspray        5   2669   533.8    34.7 <2e-16 ***\nResiduals   66   1015    15.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncheck_normality(aov2)\n\nWarning: Non-normality of residuals detected (p = 0.022).\n\ncheck_heteroscedasticity(aov2)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p < .001).\n\n\nInterpretação: dados não são normais e homogeneos.\n\n\nQuando se tem dados não paramétricos, tem-se 3 alternativas - transformar os dados (raiz quadrada, log, etc); usar testes não paramétricos (Kruskal) ou usar modelos lineares generalizados (melhor opção).\n1. Transformar os dados para normalizar: Usa-se a raiz quadrada para tentar noprmalizar e tornar os dados normais e homogenos. Pode-se também tentar o log da variável resposta + 0.5.\n\naov2 <- aov(sqrt(count) ~ spray, data = insects)\nsummary(aov2)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \nspray        5  88.44  17.688    44.8 <2e-16 ***\nResiduals   66  26.06   0.395                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncheck_normality(aov2)\n\nOK: residuals appear as normally distributed (p = 0.681).\n\ncheck_heteroscedasticity(aov2)\n\nOK: Error variance appears to be homoscedastic (p = 0.854).\n\n\n2. Uso de testes não paramétricos: Se não normalizar e os dados ainda forem heterogenos, usa-se testes não paramétricos. Uma das saídas para normalizar os dados é a utilização do teste de Kruskal-Wallis. O teste de Kruskal-Wallis utiliza os valores numéricos transformados em postos e agrupados num só conjunto de dados, é testado se as amostras vêm de uma mesma população, ou se pelo menos uma delas vêm de população distinta das demais. O teste de Kruskal-Wallis dispensa a pressuposição de normalidade e homocedasticidade. Tem 2 opções de teste Kruskal. Para usar essa opção, é necessário baixar e carregar o pacote agricolae.\n\nkruskal.test(count ~ spray, data = insects)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  count by spray\nKruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10\n\nlibrary(agricolae)\nkruskal(insects$count, insects$spray, \n        console = TRUE)\n\n\nStudy: insects$count ~ insects$spray\nKruskal-Wallis test's\nTies or no Ties\n\nCritical Value: 54.69134\nDegrees of freedom: 5\nPvalue Chisq  : 1.510845e-10 \n\ninsects$spray,  means of the ranks\n\n  insects.count  r\nA      52.16667 12\nB      54.83333 12\nC      11.45833 12\nD      25.58333 12\nE      19.33333 12\nF      55.62500 12\n\nPost Hoc Analysis\n\nt-Student: 1.996564\nAlpha    : 0.05\nMinimum Significant Difference: 8.462804 \n\nTreatments with the same letter are not significantly different.\n\n  insects$count groups\nF      55.62500      a\nB      54.83333      a\nA      52.16667      a\nD      25.58333      b\nE      19.33333     bc\nC      11.45833      c\n\n\nPacote emmeans\nO pacote emmeans (“estimated marginal means”, ou médias marginais estimadas) é usado para realizar testes de comparação de médias entre grupos, ajustando para outros fatores importantes que podem influenciar as médias. O pacote é particularmente útil em modelos lineares generalizados (GLM).\nFunção emmeans - tirar a média da variável inseticida: Para dar o valor original da média e não o valor transformado, usa-se a função type = response. A função pwpm gera uma tabela de comparação das médias e cld é uma função que serve para gerar os números que diferenciam os grupos de médias.\n\naov2 <- aov(count ~ spray, data = insects)\nsummary(aov2)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \nspray        5   2669   533.8    34.7 <2e-16 ***\nResiduals   66   1015    15.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncheck_normality(aov2)\n\nWarning: Non-normality of residuals detected (p = 0.022).\n\ncheck_heteroscedasticity(aov2)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p < .001).\n\nlibrary(emmeans)\naov2_means <- emmeans(aov2, ~ spray,\n                         type = \"response\")\naov2_means\n\n spray emmean   SE df lower.CL upper.CL\n A      14.50 1.13 66   12.240    16.76\n B      15.33 1.13 66   13.073    17.59\n C       2.08 1.13 66   -0.177     4.34\n D       4.92 1.13 66    2.656     7.18\n E       3.50 1.13 66    1.240     5.76\n F      16.67 1.13 66   14.406    18.93\n\nConfidence level used: 0.95 \n\npwpm(aov2_means)\n\n        A       B       C       D       E       F\nA [14.50]  0.9952  <.0001  <.0001  <.0001  0.7542\nB  -0.833 [15.33]  <.0001  <.0001  <.0001  0.9603\nC  12.417  13.250 [ 2.08]  0.4921  0.9489  <.0001\nD   9.583  10.417  -2.833 [ 4.92]  0.9489  <.0001\nE  11.000  11.833  -1.417   1.417 [ 3.50]  <.0001\nF  -2.167  -1.333 -14.583 -11.750 -13.167 [16.67]\n\nRow and column labels: spray\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean)   type = \"response\"\nLower triangle: Comparisons (estimate)   earlier vs. later\n\nlibrary(multcomp)\nlibrary(multcompView)\ncld(aov2_means)\n\n spray emmean   SE df lower.CL upper.CL .group\n C       2.08 1.13 66   -0.177     4.34  1    \n E       3.50 1.13 66    1.240     5.76  1    \n D       4.92 1.13 66    2.656     7.18  1    \n A      14.50 1.13 66   12.240    16.76   2   \n B      15.33 1.13 66   13.073    17.59   2   \n F      16.67 1.13 66   14.406    18.93   2   \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n3. GLM: A terceira opção é a geração de modelos lineares generalizados. Para publicação de artigos, essa é a opção mais aconselhável, é mais elegante do que transformar os dados. Para a geração de modelos, a função a ser utilizada é a glm e precisa indicar os argumentos family = poisson(link = “identity”). Para visualizar, pode usar o pacote Dharma e puxar um plot.\n\nglm1 <- glm(count ~spray,\n             data = insects,\n             family = poisson(link = \"identity\"))\nplot(simulateResiduals(glm1))\n\n\n\nsummary(glm1)\n\n\nCall:\nglm(formula = count ~ spray, family = poisson(link = \"identity\"), \n    data = insects)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3852  -0.8876  -0.1482   0.6063   2.6922  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  14.5000     1.0992  13.191  < 2e-16 ***\nsprayB        0.8333     1.5767   0.529    0.597    \nsprayC      -12.4167     1.1756 -10.562  < 2e-16 ***\nsprayD       -9.5833     1.2720  -7.534 4.92e-14 ***\nsprayE      -11.0000     1.2247  -8.981  < 2e-16 ***\nsprayF        2.1667     1.6116   1.344    0.179    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 409.041  on 71  degrees of freedom\nResidual deviance:  98.329  on 66  degrees of freedom\nAIC: 376.59\n\nNumber of Fisher Scoring iterations: 3\n\nglm1_means <- emmeans(glm1, ~ spray)\ncld(glm1_means)\n\n spray emmean    SE  df asymp.LCL asymp.UCL .group\n C       2.08 0.417 Inf      1.27      2.90  1    \n E       3.50 0.540 Inf      2.44      4.56  12   \n D       4.92 0.640 Inf      3.66      6.17   2   \n A      14.50 1.099 Inf     12.35     16.65    3  \n B      15.33 1.130 Inf     13.12     17.55    3  \n F      16.67 1.179 Inf     14.36     18.98    3  \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same."
  },
  {
    "objectID": "Aula11.html",
    "href": "Aula11.html",
    "title": "Anova Fatorial",
    "section": "",
    "text": "Quando se tem dois fatores ou mais, não se tem a opção de usar teste não paramétrico (Kruskal). A função aov() em 1 fator é apenas variável resposta em função do tratamento (resposta ~ tratamento), mas com 2 ou mais o aov() é a resposta ~ do tratamento1*tratamento2 (ex.: resposta em função do fator 1 versus o fator2).\nPreparo pré-análise:\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nBanco de dados utilizado: fungicida-vaso (conjunto de dados do dados diversos). Objeto nomeado como dat.\n\ndat <- read_excel(\"dados-diversos.xlsx\", \"fungicida_vaso\")\n\nCalculo da % do número de espigas com doença. Cria uma nova coluna:\n\ndat2 <- dat |>\n  mutate(inc = dis_sp / n_sp*100)\n\ndat2 |>\n  ggplot(aes(x = treat,\n             y = inc))+\n  geom_jitter(width = 0.1)+\n  facet_wrap(~dose)\n\n\n\n\nGera modelo de anova: trata como dados qualitativos.\n\nm1 <- aov(inc ~treat*dose, \n          dat = dat2)\nsummary(m1)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ntreat        1  919.5   919.5   24.31 0.000151 ***\ndose         1  920.9   920.9   24.34 0.000150 ***\ntreat:dose   1  747.7   747.7   19.76 0.000407 ***\nResiduals   16  605.3    37.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheca as premissas e visualiza com DHARMa:\n\nlibrary(performance)\ncheck_normality(m1)\n\nWarning: Non-normality of residuals detected (p = 0.018).\n\ncheck_heteroscedasticity(m1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p < .001).\n\nlibrary(DHARMa)\nplot(simulateResiduals(m1))\n\n\n\n\nComo está se trabalhando com dados de percentagem,tenta transformar os dados com log. Embora a normalidade pareça anormal, a homogeneidade é mais importante.\n\nm1 <- aov(log(inc + 0.5) ~ treat*dose, \n          dat = dat2)\nsummary(m1)\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \ntreat        1 12.928  12.928  13.980 0.00179 **\ndose         1  5.663   5.663   6.124 0.02491 * \ntreat:dose   1  5.668   5.668   6.129 0.02486 * \nResiduals   16 14.796   0.925                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncheck_normality(m1)\n\nWarning: Non-normality of residuals detected (p = 0.050).\n\ncheck_heteroscedasticity(m1)\n\nOK: Error variance appears to be homoscedastic (p = 0.180).\n\nplot(simulateResiduals(m1))\n\n\n\n\nEstimando a média dos tratamentos para cada interação: primeiro os tratamentos dentro das doses e depois as dose dentro dos tratamentos.\n\nlibrary(emmeans)\nmeans_m1 <- emmeans(m1, ~ treat | dose,\n                    type = \"response\")\nmeans_m1\n\ndose = 0.5:\n treat        response     SE df lower.CL upper.CL\n Ionic liquid    27.05 11.847 16   10.570    68.05\n Tebuconazole     1.21  0.737 16    0.188     3.76\n\ndose = 2.0:\n treat        response     SE df lower.CL upper.CL\n Ionic liquid     3.10  1.412 16    1.065     7.77\n Tebuconazole     1.42  0.925 16    0.194     4.83\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log(mu + 0.5) scale \n\nlibrary(multcompView)\nlibrary(multcomp)\ncld(means_m1)\n\ndose = 0.5:\n treat        response     SE df lower.CL upper.CL .group\n Tebuconazole     1.21  0.737 16    0.188     3.76  1    \n Ionic liquid    27.05 11.847 16   10.570    68.05   2   \n\ndose = 2.0:\n treat        response     SE df lower.CL upper.CL .group\n Tebuconazole     1.42  0.925 16    0.194     4.83  1    \n Ionic liquid     3.10  1.412 16    1.065     7.77  1    \n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log(mu + 0.5) scale \nTests are performed on the log scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n#dose dentro dos tratamentos \n#means_ml <- emmeans(ml1, ~ dose | trat, type = \"response\")\n\nVendo o coeficiente de variação:\n\nlibrary(agricolae)\ncv.model(m1)\n\n[1] 65.04818\n\n\n\n\nDados sobre a interação entre tipo de armazenamento e umidade.\n\nmilho <- read_excel(\"dados-diversos.xlsx\", \"armazena\")\nmilho |>\n  filter(tempo ==8) |>\n  ggplot(aes(factor(tipo), peso_mil,\n             color = factor(umidade)))+\n  geom_jitter(width = 0.1)+\n  facet_wrap(~ umidade)\n\n\n\n\nTestar a interação entre o tipo de armazenamento e o tempo 8:\n\nmilho2 <- milho |>\n  filter(tempo ==8)\n\nm2 <- aov(peso_mil ~ factor(tipo)*factor(umidade),\n          data = milho2)\nsummary(m2)\n\n                             Df Sum Sq Mean Sq F value   Pr(>F)    \nfactor(tipo)                  1  11215   11215  2375.8 3.64e-15 ***\nfactor(umidade)               2  42814   21407  4534.8  < 2e-16 ***\nfactor(tipo):factor(umidade)  2   2329    1165   246.7 1.79e-10 ***\nResiduals                    12     57       5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\ntestanto tipo de inoculação na incidencia de fusarium em milho\n\nmilho3 <- read_excel(\"dados-diversos.xlsx\", \"milho\")\n\nm4 <- aov(yield ~hybrid*method,\n          data = milho3)\nsummary(m4)\n\n              Df    Sum Sq  Mean Sq F value   Pr(>F)    \nhybrid         5 105876446 21175289   8.312 2.66e-05 ***\nmethod         1     42951    42951   0.017    0.897    \nhybrid:method  5  10619453  2123891   0.834    0.534    \nResiduals     36  91709593  2547489                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncheck_heteroscedasticity(m4)\n\nOK: Error variance appears to be homoscedastic (p = 0.928).\n\nplot(simulateResiduals(m4))\n\n\n\nmedias_m4 <- emmeans(m4, ~ hybrid)\nmedias_m4\n\n hybrid   emmean  SE df lower.CL upper.CL\n 30F53 HX  10598 564 36     9453    11742\n 30F53 YH   9309 564 36     8165    10454\n 30K64     11018 564 36     9874    12162\n 30S31H     8652 564 36     7507     9796\n 30S31YH    8056 564 36     6912     9201\n BG7049H   12402 564 36    11257    13546\n\nResults are averaged over the levels of: method \nConfidence level used: 0.95 \n\ncld(medias_m4)\n\n hybrid   emmean  SE df lower.CL upper.CL .group\n 30S31YH    8056 564 36     6912     9201  1    \n 30S31H     8652 564 36     7507     9796  12   \n 30F53 YH   9309 564 36     8165    10454  12   \n 30F53 HX  10598 564 36     9453    11742   23  \n 30K64     11018 564 36     9874    12162   23  \n BG7049H   12402 564 36    11257    13546    3  \n\nResults are averaged over the levels of: method \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nCaso a interação não dê sifnificativa, tira a interação e deixa só o fator que teve significancia (isola o fator).\n\nm5 <- aov(yield ~hybrid, data = milho3)\nsummary(m5)\n\n            Df    Sum Sq  Mean Sq F value   Pr(>F)    \nhybrid       5 105876446 21175289   8.688 1.02e-05 ***\nResiduals   42 102371996  2437428                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm4 <- aov(yield ~hybrid,\n          data = milho3)\nsummary(m5)\n\n            Df    Sum Sq  Mean Sq F value   Pr(>F)    \nhybrid       5 105876446 21175289   8.688 1.02e-05 ***\nResiduals   42 102371996  2437428                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncheck_heteroscedasticity(m5)\n\nOK: Error variance appears to be homoscedastic (p = 0.763).\n\nplot(simulateResiduals(m5))\n\n\n\nmedias_m5 <- emmeans(m5, ~hybrid)\nmedias_m5\n\n hybrid   emmean  SE df lower.CL upper.CL\n 30F53 HX  10598 552 42     9484    11712\n 30F53 YH   9309 552 42     8195    10423\n 30K64     11018 552 42     9904    12132\n 30S31H     8652 552 42     7538     9765\n 30S31YH    8056 552 42     6942     9170\n BG7049H   12402 552 42    11288    13516\n\nConfidence level used: 0.95 \n\ncld(medias_m5)\n\n hybrid   emmean  SE df lower.CL upper.CL .group\n 30S31YH    8056 552 42     6942     9170  1    \n 30S31H     8652 552 42     7538     9765  12   \n 30F53 YH   9309 552 42     8195    10423  123  \n 30F53 HX  10598 552 42     9484    11712   234 \n 30K64     11018 552 42     9904    12132    34 \n BG7049H   12402 552 42    11288    13516     4 \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 6 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\npwpm(medias_m5)\n\n         30F53 HX 30F53 YH   30K64  30S31H 30S31YH BG7049H\n30F53 HX  [10598]   0.5709  0.9942  0.1494  0.0254  0.2125\n30F53 YH     1288  [ 9309]  0.2643  0.9576  0.5999  0.0036\n30K64        -420    -1709 [11018]  0.0447  0.0059  0.4938\n30S31H       1946      658    2366 [ 8652]  0.9723  0.0003\n30S31YH      2541     1253    2962     595 [ 8056]  <.0001\nBG7049H     -1804    -3092   -1384   -3750   -4345 [12402]\n\nRow and column labels: hybrid\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later"
  },
  {
    "objectID": "Aula12.html",
    "href": "Aula12.html",
    "title": "ANOVA em DBC e parcela subdividida",
    "section": "",
    "text": "Preparo pré-análise:\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nUsando o conjunto de dados fungicida_campo:\n\nfungicidas <- read_excel(\"dados-diversos.xlsx\", \"fungicida_campo\")\n\n\n\nNo delineamento em blocos, adiciona o sinal de + para adicionar a repetição.\n\naov_fung <- aov(sev ~trat + rep, data = fungicidas)\nsummary(aov_fung)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \ntrat         7   7135  1019.3 287.661 <2e-16 ***\nrep          1     19    18.6   5.239 0.0316 *  \nResiduals   23     81     3.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nChecando as premissas:\n\nlibrary(performance)\nlibrary(DHARMa)\ncheck_normality(aov_fung)\n\nOK: residuals appear as normally distributed (p = 0.230).\n\ncheck_heteroscedasticity(aov_fung)\n\nOK: Error variance appears to be homoscedastic (p = 0.484).\n\nplot(simulateResiduals(aov_fung))\n\n\n\n\nEstimando as médias:\n\nlibrary(emmeans)\nmeans_fung <- emmeans(aov_fung, ~trat)\nlibrary(multcomp)\nlibrary(multcompView)\ncld(means_fung)\n\n trat       emmean    SE df lower.CL upper.CL .group\n G            29.2 0.941 23     27.3     31.2  1    \n B            29.5 0.941 23     27.6     31.4  1    \n E            30.1 0.941 23     28.2     32.1  1    \n C            30.4 0.941 23     28.4     32.3  1    \n A            30.4 0.941 23     28.4     32.3  1    \n D            31.5 0.941 23     29.6     33.4  12   \n F            35.5 0.941 23     33.6     37.4   2   \n testemunha   75.8 0.941 23     73.8     77.7    3  \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 8 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\nplot(means_fung)+\n  coord_flip()+\n  theme_minimal()"
  },
  {
    "objectID": "Aula13.html",
    "href": "Aula13.html",
    "title": "Análise de Regressão",
    "section": "",
    "text": "Quando se tem dados quantitativos, a análise de regressão é geralmente mais apropriada do que a análise de variância (ANOVA), pois a análise de regressão permite modelar e prever a relação entre uma variável dependente (Y) e uma ou mais variáveis independentes (X). Ela busca estimar os parâmetros de uma equação de regressão que descreve a relação funcional entre as variáveis. A análise de regressão pode fornecer informações sobre a direção e magnitude do efeito das variáveis independentes na variável dependente, bem como previsões ou estimativas para valores não observados.\n\n\nNa análise de regressão linear, assume-se que a relação entre a variável dependente e a variável independente é linear, ou seja, pode ser descrita por uma linha reta. A equação da regressão linear é dada por: y = β0 + β1x + ε, onde>: y é a variável dependente (variável resposta), x é a variável independente (tratamento), β0 é o intercepto (ou constante) da equação de regressão, β1 é o coeficiente de inclinação (slope) da equação de regressão e ε é o erro aleatório.\nNa regressão linear simples, testamos a hipótese de que a reta de regressão (linha de melhor ajuste), que representa a relação entre as variáveis independentes e a variável dependente, tem um coeficiente de inclinação diferente de zero. Ou seja, testamos a hipótese de que a inclinação da reta de regressão é significativamente diferente de zero (testa se o p valor é diferente de 0).\nPreparo pré-análise:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\nConjunto de dados estande (dados-diversos): Visualização em ggplot Para ajustar para uma regressão linear usa-se o argumento method = “lm” dentro da função geom_smooth.\n\nestande <- read_excel(\"dados-diversos.xlsx\", \"estande\")\nestande |>\n  ggplot(aes(trat, nplants))+\n  geom_point()+\n  facet_wrap(~ exp)+\n  geom_smooth(se =  F, method = \"lm\")\n\n\n\n\n\n\nPosteriomente, deve-se testar o modelo que melhor se ajusta aos dados.Pode-se testar fazer a análise de regressão para cada experimento (isola cada experimento) ou analisar em grupos (modelos mistos).\nAnalisando cada experimento isoladamente Para isso, cria um novo objeto para os dados (exp1) e atribui estande a ele, depois deve-se filtrar o experimento que deseja e criar um objeto para esse conjunto para viabilizar a realização da análise de regressão.\nExp 1:\n\nexp1 <- estande |>\n  filter(exp == 1)\n\nm1 <- lm(nplants ~trat, data = exp1)\nsummary(m1)\n\n\nCall:\nlm(formula = nplants ~ trat, data = exp1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.500  -6.532   1.758   8.573  27.226 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  52.5000     4.2044  12.487 1.84e-11 ***\ntrat         -0.2419     0.1859  -1.301    0.207    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15 on 22 degrees of freedom\nMultiple R-squared:  0.07148,   Adjusted R-squared:  0.02928 \nF-statistic: 1.694 on 1 and 22 DF,  p-value: 0.2066\n\n\nO valor do intercept e o valor de trat (slope) são utilizados na tabela. Intercept é o valor da variável dependente quando a variável independente é igual a zero. Já o slope é a medida da inclinação da linha de regressão, que representa a mudança na variável dependente associada a uma mudança na variável independente.\nPara o experimento 2:\n\nexp2 <- estande |>\n  filter(exp == 2)\n\nm2 <- lm(nplants ~trat, data = exp2)\nsummary(m2)\n\n\nCall:\nlm(formula = nplants ~ trat, data = exp2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.7816  -7.7150   0.5653   8.1929  19.2184 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  60.9857     3.6304  16.798 4.93e-14 ***\ntrat         -0.7007     0.1605  -4.365 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.95 on 22 degrees of freedom\nMultiple R-squared:  0.4641,    Adjusted R-squared:  0.4398 \nF-statistic: 19.05 on 1 and 22 DF,  p-value: 0.0002473\n\n\nPara o exp3:\n\nexp3 <- estande |>\n  filter(exp == 3)\n\nm3 <- lm(nplants ~trat, data = exp3)\nsummary(m3)\n\n\nCall:\nlm(formula = nplants ~ trat, data = exp3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.5887  -3.9597   0.7177   5.5806  19.8952 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  95.7500     2.9529  32.425  < 2e-16 ***\ntrat         -0.7634     0.1306  -5.847 6.97e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.53 on 22 degrees of freedom\nMultiple R-squared:  0.6085,    Adjusted R-squared:  0.5907 \nF-statistic: 34.19 on 1 and 22 DF,  p-value: 6.968e-06\n\nlibrary(report)\nreport(m3)\n\nWe fitted a linear model (estimated using OLS) to predict nplants with trat\n(formula: nplants ~ trat). The model explains a statistically significant and\nsubstantial proportion of variance (R2 = 0.61, F(1, 22) = 34.19, p < .001, adj.\nR2 = 0.59). The model's intercept, corresponding to trat = 0, is at 95.75 (95%\nCI [89.63, 101.87], t(22) = 32.43, p < .001). Within this model:\n\n  - The effect of trat is statistically significant and negative (beta = -0.76,\n95% CI [-1.03, -0.49], t(22) = -5.85, p < .001; Std. beta = -0.78, 95% CI\n[-1.06, -0.50])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nGráfico para representar a regressão - Para unir os 3 graficos, usa o patchwork.\n\ng1 <- exp1 |>\n  ggplot(aes(trat, nplants))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = F)+\n  annotate(geom = \"text\", x = 24,\n           y = 70, label = \"y = 52.5 - 0.24x\")\n\ng2 <- exp2 |>\n  ggplot(aes(trat, nplants))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = F)+\n  annotate(geom = \"text\", x = 24,\n           y = 70, label = \"y = 60 - 0.07x\")\ng3 <- exp3 |>\n  ggplot(aes(trat, nplants))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = F)+\n  annotate(geom = \"text\", x = 24,\n           y = 70, label = \"y = 95 - 0.07x\")\n\nlibrary(patchwork)\ng1|g2|g3\n\n\n\n\n\n\n\n\nEm um modelo misto, as observações são divididas em grupos ou subgrupos. Cada grupo pode ter um conjunto diferente de efeitos aleatórios e/ou fixos, dependendo da estrutura dos dados. Por exemplo, se os dados foram coletados em diferentes locais geográficos, podemos ter um efeito aleatório para cada local (como no caso do conjunto de dados estande).\nEquação: b1 - b2x - cx2\n\nlibrary(lme4)\nmix <- lmer(nplants ~trat + (trat | exp),\n            data = estande)\nsummary(mix)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: nplants ~ trat + (trat | exp)\n   Data: estande\n\nREML criterion at convergence: 580.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.0988 -0.6091  0.1722  0.6360  1.9963 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n exp      (Intercept) 510.68405 22.5983       \n          trat          0.05516  0.2349  -0.82\n Residual             167.91303 12.9581       \nNumber of obs: 72, groups:  exp, 3\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  69.7452    13.2146   5.278\ntrat         -0.5687     0.1643  -3.462\n\nCorrelation of Fixed Effects:\n     (Intr)\ntrat -0.731\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00274249 (tol = 0.002, component 1)\n\nlibrary(car)\nAnova(mix)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: nplants\n      Chisq Df Pr(>Chisq)    \ntrat 11.985  1  0.0005362 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuando se usa o modelo misto, considera que todos os experimentos são agrupados, então considera que amostra é aleatória. Para fazer o modelo de regressão em grupo (misto) acrescenta-se na função aestetic o argumento group = exp.\n\nestande <- read_excel(\"dados-diversos.xlsx\", \"estande\")\nestande |>\n  ggplot(aes(trat, nplants, group = exp))+\n  geom_point()+\n  #facet_wrap(~ exp)+\n  geom_smooth(se =  F, method = \"lm\")\n\n\n\n\nEm geral, os modelos mistos são mais poderosos do que os modelos que tratam cada experimento isoladamente, pois levam em conta a variação tanto entre quanto dentro dos experimentos. Além disso, os modelos mistos permitem que os dados sejam analisados em sua totalidade, sem perder informações importantes sobre a estrutura dos dados.\n\n\n\nO modelo linear generalizado é uma alternativa ao modelo linear. O GLM é considerado uma extensão do modelo linear que permite acomodar uma variedade de tipos de variáveis resposta, incluindo variáveis categóricas e contínuas. Além disso, o modelo linear generalizado permite que a relação entre a variável resposta e as variáveis explicativas seja não-linear. Ou seja, ele não se limita à suposição de que a relação é uma linha reta.\n\nlm1 <- lm(nplants ~ trat, data = exp3)\nsummary(lm1)\n\n\nCall:\nlm(formula = nplants ~ trat, data = exp3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.5887  -3.9597   0.7177   5.5806  19.8952 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  95.7500     2.9529  32.425  < 2e-16 ***\ntrat         -0.7634     0.1306  -5.847 6.97e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.53 on 22 degrees of freedom\nMultiple R-squared:  0.6085,    Adjusted R-squared:  0.5907 \nF-statistic: 34.19 on 1 and 22 DF,  p-value: 6.968e-06\n\nglm1 <- glm(nplants ~ trat, family = \"gaussian\",\n            data = exp3)\n\nglm2 <- glm(nplants ~ trat, family = poisson(link = \"log\"),\n            data = exp3)\nAIC(glm1)\n\n[1] 185.0449\n\nAIC(glm2)\n\n[1] 183.9324\n\nsummary(glm1)\n\n\nCall:\nglm(formula = nplants ~ trat, family = \"gaussian\", data = exp3)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-26.5887   -3.9597    0.7177    5.5806   19.8952  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  95.7500     2.9529  32.425  < 2e-16 ***\ntrat         -0.7634     0.1306  -5.847 6.97e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 110.9787)\n\n    Null deviance: 6235.8  on 23  degrees of freedom\nResidual deviance: 2441.5  on 22  degrees of freedom\nAIC: 185.04\n\nNumber of Fisher Scoring iterations: 2\n\nsummary(glm2)\n\n\nCall:\nglm(formula = nplants ~ trat, family = poisson(link = \"log\"), \n    data = exp3)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.94600  -0.46988   0.02453   0.61868   2.34657  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  4.571590   0.029539 154.762  < 2e-16 ***\ntrat        -0.009965   0.001488  -6.697 2.13e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 77.906  on 23  degrees of freedom\nResidual deviance: 29.952  on 22  degrees of freedom\nAIC: 183.93\n\nNumber of Fisher Scoring iterations: 4\n\n\nO modelo linear generalizado com distribuição gaussiana (family = gaussian) é usado quando a variável resposta é contínua e tem uma distribuição normal, ou seja, o mesmo que “lm”. Já o modelo linear generalizado com distribuição de Poisson é usado quando a variável resposta é um número inteiro e não negativo e segue uma distribuição de Poisson (family = poisson). O AIC (Akaike’s Information Criterion) é um critério de seleção de modelo usado para escolher o melhor modelo entre vários modelos candidatos. A escolha do modelo com o menor AIC é importante porque ele é uma medida de qualidade do modelo que leva em consideração tanto o ajuste aos dados quanto a complexidade do modelo. Como o AIC de menor valor é melhor, a família poisson é o ideal para os dados analisados."
  },
  {
    "objectID": "Aula14.html",
    "href": "Aula14.html",
    "title": "Análise de Correlação",
    "section": "",
    "text": "A análise de correlação é usada para avaliar a força e a direção da relação entre duas variáveis contínuas. Ela mede o grau de associação linear entre as variáveis, ou seja, o quanto as variáveis se movem juntas em uma relação linear. A medida mais comum de correlação é o coeficiente de correlação de Pearson, que varia de -1 a +1. Um valor próximo de +1 indica uma forte correlação positiva, um valor próximo de -1 indica uma forte correlação negativa, e um valor próximo de zero indica uma correlação fraca ou inexistente. A análise de correlação não implica causalidade, apenas indica a presença e a intensidade da relação linear entre as variáveis.\nPreparo pré-análise e visualização: Pacotes, conjunto de dados e gráfico.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\n\nestande <- read_excel(\"dados-diversos.xlsx\", \"estande\")\n\nestande |>\n  ggplot(aes(trat, nplants))+\n  geom_point()+\n  facet_wrap(~ exp)+\n  ylim(0,max(estande$nplants))+\n  geom_smooth(se =  F)\n\n\n\n\n\n\nUtiliza-se a função lm() para ajustar o modelo linear. Essa função recebe a fórmula do modelo e os dados como argumentos. A fórmula especifica a relação entre a variável dependente e a variável independente. Por exemplo, se sua variável dependente for “y” e sua variável independente for “x”, a fórmula seria y ~ x. A função lm() retornará um objeto do tipo “lm” que representa o modelo ajustado. Usa-se a função summary() para obter uma visão geral dos resultados do modelo ajustado. Isso inclui os coeficientes estimados, estatísticas de ajuste (como o coeficiente de determinação R²), valores-p e outros diagnósticos do modelo.\n\nVer qual que se adapta melhor.\n\nFiltrar o experimento 2: O group_by vai agrupar pelos tratamentos e o summarise vai dar a média do número de plantas. Se eu quiser seguir a linha em cima dos pontos eu posso colocar o argumento span = 0.7 dentro da função geom_smooth. O R ao quadrado da regressão linear simples é o coeficiente de determinação.\n\nestande2 <- estande |>\n  filter(exp ==2) |>\n  group_by(trat) |>\n  summarise(mean_nplants = mean(nplants))\n  \nestande2|>\n  ggplot(aes(trat, mean_nplants))+\n  geom_point()+\n  #geom_line()\n  geom_smooth(formula = y ~ poly(x, 2), method = \"lm\", color = \"black\")+\n  annotate(geom = \"text\", \n           x = 25, y = 70,\n           label = \"y = 66.3 - 1.777x + 0.0222x2\n           R2 = 0.0.88\")\n\n\n\n\nOlhando o ajuste do modelo quadrático:\nPara ajustar os dados a um modelo linear quadrático, basta utilizar a função lm(). A diferença é que precisará adicionar a variável independente ao quadrado na fórmula do modelo. Por exemplo, se sua variável dependente for “y” e sua variável independente for “x”, a fórmula seria y ~ x + I(x^2). A função I() é usada para indicar uma operação especial, neste caso, elevar “x” ao quadrado. A função lm() retornará um objeto do tipo “lm” que representa o modelo ajustado.\n\nestande2 <- estande2 |>\n  mutate(trat2 = trat^2)\n  m1 <- lm(mean_nplants ~ trat, data = estande2)\nsummary(m1)\n\n\nCall:\nlm(formula = mean_nplants ~ trat, data = estande2)\n\nResiduals:\n     1      2      3      4      5      6 \n12.764 -2.134 -6.782 -3.327 -4.669  4.147 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  60.9857     4.5505  13.402 0.000179 ***\ntrat         -0.7007     0.2012  -3.483 0.025294 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.117 on 4 degrees of freedom\nMultiple R-squared:  0.752, Adjusted R-squared:   0.69 \nF-statistic: 12.13 on 1 and 4 DF,  p-value: 0.02529\n\nhist(m1$residuals)\n\n\n\nm2 <- lm(mean_nplants ~ trat + trat2,\n         data = estande2)\nsummary(m2)\n\n\nCall:\nlm(formula = mean_nplants ~ trat + trat2, data = estande2)\n\nResiduals:\n      1       2       3       4       5       6 \n 7.4484 -4.4200 -6.4386  1.0739  3.0474 -0.7111 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 66.30156    4.70800  14.083 0.000776 ***\ntrat        -1.77720    0.62263  -2.854 0.064878 .  \ntrat2        0.02223    0.01242   1.790 0.171344    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.517 on 3 degrees of freedom\nMultiple R-squared:  0.8801,    Adjusted R-squared:  0.8001 \nF-statistic: 11.01 on 2 and 3 DF,  p-value: 0.04152\n\nAIC(m1, m2)\n\n   df      AIC\nm1  3 45.72200\nm2  4 43.36151"
  },
  {
    "objectID": "Aula15.html",
    "href": "Aula15.html",
    "title": "Tabela de contingência",
    "section": "",
    "text": "Variáveis categóricas e tabelas de contingência\nPara variaveis resposta categoricas, não se tira a média dos dados, ao inves disso, se monta uma tabela de contigencia e visualiza por meio de grafico de barras/colunas, pois se é sada para ver frequencia de ocorrencias, quando se tem variáveis categóricas nominais.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(janitor)\nlibrary(epifitter)\n\nDados: Quantas ocorrências tem em cada ano. Cruzando duas variáveis. Quantas espécies dos dados tem em cada ano (função table)\n\nsurvey <- read_excel(\"dados-diversos.xlsx\",\"survey\")\nsurvey |>\n  count(year)\n\n# A tibble: 3 × 2\n   year     n\n  <dbl> <int>\n1  2009   265\n2  2010   216\n3  2011   185\n\nq <- table(survey$residue, survey$species)\n\n\n#library(janitor)\nsurvey |>\n  tabyl(year, species)\n\n year Fgra Fspp\n 2009  225   40\n 2010  187   29\n 2011  140   45\n\n\nPacote janitor: Para dar os valores em porcentagem usa-se a função adorn_percentages().\n\nsurvey |>\n  filter(residue != \"NA\") |>\n  count(residue, species) |>\n  ggplot(aes(residue, n, fill = species))+\n  geom_col()\n\n\n\n\nTeste q quadrado:\n\nq <- table(survey$residue, survey$species)\nchisq.test(q)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  q\nX-squared = 1.1997, df = 1, p-value = 0.2734\n\n\nHipótese nula: as populações são iguais.\nUsa-se o fisher.test(q) quando o número de observações é baixo, algo em torno de 6 ou 7.\nA severidade é influenciada pelo resíduo?\n\nq <- table(survey$residue, survey$inc_class)\nchisq.test(q)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  q\nX-squared = 2.6165, df = 1, p-value = 0.1058\n\nsurvey|>\n  filter(residue != \"NA\") |>\n  count(residue, inc_class) |>\n  ggplot(aes(residue, n, fill = inc_class))+\n  geom_col()\n\n\n\n\nPelo p valor, a classe de severidade independe do resto cultura, ou seja, o resto cultural não influencia na severidade.\nNovo conjunto de dados:\n\ncurve <- read_excel(\"dados-diversos.xlsx\", \"curve\")\n\ncurve2 <- curve |> \n  group_by(Irrigation, day) |>\n  summarize(mean_severity = mean(severity),\n            sd_severity = sd(severity))\n\n  curve2 |>\n    ggplot(aes(day, mean_severity, color = Irrigation))+\n    geom_point()+\n    geom_errorbar(aes(ymin = mean_severity - sd_severity, ymax = mean_severity + sd_severity, width = 0.01))+\n    geom_line()\n\n\n\n  curve3 <- curve |>\n    group_by(Irrigation, rep) |>\n    summarise(audpc = AUDPC(day, severity, \n                            y_proportion = F)) |>\n    pivot_wider(1, names_from = Irrigation,\n              values_from = audpc)\n  \n  t.test(curve3$Drip, curve$Furrow)\n\n\n    One Sample t-test\n\ndata:  curve3$Drip\nt = 51.206, df = 2, p-value = 0.0003812\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 12.26473 14.51493\nsample estimates:\nmean of x \n 13.38983 \n\n\n\n\nExercício prático:\n\nCarregando o conjunto de dados, mutando a variável lesion size de categórica para numerica, agrupando as variáveis e resumindo os dados:\n\n\nlesion_size <- read_excel(\"tan-spot-wheat.xlsx\", \"lesion_size\")\n\nlesion2 <- lesion_size |>\nmutate(lesion_size = as.numeric(lesion_size)) |>\ngroup_by(cult, silicio, hai) |>\nsummarise(mean_lesion = mean(lesion_size), sd_lesion = sd(lesion_size))\n\nVisualizando os dados graficamente:\n\nlesion2 |>\n  ggplot(aes(hai, mean_lesion, color = silicio))+\n  geom_line()+\n  geom_point()+\n    geom_errorbar(aes(ymin = mean_lesion - sd_lesion, \n                     ymax = mean_lesion + sd_lesion), \n                     width = 0.01)+\n    facet_wrap(~cult)+\n  labs(y = \"Lesion size (mm)\",\n       x = \"Hours after inoculation (hai)\", color = \"Treatment\")\n\n\n\n\n\n\nAnálise de área abaixo da curva da doença\n\nlesion3 <- lesion_size |>\n  mutate(lesion_size = as.numeric(lesion_size)) |>\n  group_by(exp, cult, silicio, rep) |>\n  summarise(audpc = AUDPC(lesion_size, hai))\n\nlesion3 |>\n  ggplot(aes(cult, audpc, color = silicio))+\n  geom_boxplot()+\n  facet_wrap(~ exp)\n\n\n\n\nTeste:\n\naov1<- aov(audpc ~ exp*cult*silicio, data = lesion3)\nsummary(aov1)\n\n                 Df  Sum Sq Mean Sq F value   Pr(>F)    \nexp               1   54544   54544   0.533 0.470257    \ncult              1  751290  751290   7.335 0.010281 *  \nsilicio           1 9051552 9051552  88.377 3.15e-11 ***\nexp:cult          1   36717   36717   0.359 0.553089    \nexp:silicio       1      49      49   0.000 0.982615    \ncult:silicio      1 1412562 1412562  13.792 0.000689 ***\nexp:cult:silicio  1  143643  143643   1.403 0.244065    \nResiduals        36 3687093  102419                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSe não der significativo, reduz o modelo:\n\naov1<- aov(audpc ~ cult*silicio, data = lesion3)\nsummary(aov1)\n\n             Df  Sum Sq Mean Sq F value   Pr(>F)    \ncult          1  751290  751290   7.662  0.00850 ** \nsilicio       1 9051552 9051552  92.315 6.04e-12 ***\ncult:silicio  1 1412562 1412562  14.406  0.00049 ***\nResiduals    40 3922047   98051                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPara ver se o modelo está correto, checa as premissas:\n\nlibrary(performance)\ncheck_normality(aov1)\n\nWarning: Non-normality of residuals detected (p = 0.003).\n\ncheck_heteroscedasticity(aov1)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.040).\n\n\nSe não atende as premissas, transforma:\n\naov1<- aov(sqrt(audpc) ~ cult*silicio, data = lesion3)\nsummary(aov1)\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \ncult          1  294.0   294.0   13.51 0.000712 ***\nsilicio       1 2363.9  2363.9  108.65 7.81e-13 ***\ncult:silicio  1  526.5   526.5   24.20 1.62e-05 ***\nResiduals    39  848.6    21.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n1 observation deleted due to missingness\n\ncheck_normality(aov1)\n\nOK: residuals appear as normally distributed (p = 0.146).\n\ncheck_heteroscedasticity(aov1)\n\nOK: Error variance appears to be homoscedastic (p = 0.638).\n\nlibrary(emmeans)\n\nm1 <- emmeans(aov1, ~ cult | silicio, type = \"response\")\nm1\n\nsilicio = Si-:\n cult      response    SE df lower.CL upper.CL\n Horizonte     1440 106.8 39     1233     1664\n Quartzo       1537 110.3 39     1322     1768\n\nsilicio = Si+:\n cult      response    SE df lower.CL upper.CL\n Horizonte      897  84.2 39      735     1075\n Quartzo        296  50.7 39      202      407\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the sqrt scale"
  },
  {
    "objectID": "Aula16.html",
    "href": "Aula16.html",
    "title": "Elaboração de mapas",
    "section": "",
    "text": "Mapas no R\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(r4pde)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthhires)\n\nInstalar pacote do github:\n\nremotes::install_github(\"ropensci/rnaturalearthhires\")\n\nConjunto de dados: Para plotar o mapa do pais, usa-se a função ne_countries\n\nsbr <- RustSoybean\n\nBRA <- ne_countries(country = \"Brazil\", \n                    returnclass = \"sf\")\nggplot(BRA) +\ngeom_sf(fill = \"white\")\n\n\n\n\nPara plotar os estados:\n\nBRA <- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nggplot(BRA) +\ngeom_sf(color = \"white\",\n          fill = \"darkgreen\") +\n  theme_void()\n\n\n\n\nPara selecionar um estado:\n\nBRA <- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nMG <- BRA |> filter(name_en == \"Minas Gerais\")\nggplot(BRA) +\ngeom_sf(color = \"black\",\n          fill = \"white\") +\n  geom_sf(data = MG, color = \"black\",\n            fill = \"green\")\n\n\n\n\nPara inserir os pontos especificos dos dados (latitude e longitude): Para plotar os pontos, precisa-se das coordenadas de onde foram coletados os pontos. Ex.: pontos de coleta - precisa-se coletar as coordenadas para plotar em um mapa (no caso de ser só o municipio, pode pegar as coordenadas na internet).\n\nBRA <- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nMG <- BRA |> filter(name_en == \"Minas Gerais\")\nggplot(BRA) +\ngeom_sf(color = \"black\",\n          fill = \"white\") +\n  geom_point(data = sbr, aes(longitude, latitude), alpha = 0.5)\n\n\n\n\nPara separar a data em dia, mês e ano:\n\nsbr2 <- sbr |>\n  separate(planting, into = \n             c(\"year\", \"month\", \"day\"), sep = \"-\", remove = FALSE)\n\nBRA <- ne_states(country = \"Brazil\", \n                    returnclass = \"sf\")\nMG <- BRA |> filter(name_en == \"Minas Gerais\")\nggplot(BRA) +\ngeom_sf(color = \"black\",\n          fill = \"white\") +\n  geom_point(data = sbr2,\n             aes(longitude, latitude, color = year), alpha = 0.5)+\n  facet_wrap(~year)+\n  theme_void()\n\n\n\n\nComo inserir a rosa dos ventos e a escala no mapa:\n\nlibrary(ggspatial)\nggplot(BRA) +\n  annotation_north_arrow(location = \"bl\")+\n  annotation_scale(location = \"br\")+\n  geom_sf(color = \"black\",\n          fill = \"white\") +\n  geom_point(data = sbr2,\n             aes(longitude, latitude, color = year, size = severity), alpha = 0.5)+\n  labs(color = \"Planting Year\")+\n  theme_minimal()+\n  theme(legend.position = \"right\")"
  },
  {
    "objectID": "Aula17.html",
    "href": "Aula17.html",
    "title": "Regressão não-linear e EC50",
    "section": "",
    "text": "Quando os dados não seguem um padrão linear, é necessário usar técnicas de regressão não linear para capturar a relação não linear entre as variáveis. A regressão não linear é usada quando a relação entre as variáveis independentes e dependentes não pode ser adequadamente modelada por uma função linear. Em outras palavras, quando os dados não seguem um padrão linear, é necessário usar técnicas de regressão não linear para capturar a relação não linear entre as variáveis.\nRegressão não-linar para determinação de EC50:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(gsheet)\nlibrary(dplyr)\n\nDados: usar o caminho do conjunto de dados sensibilidade a fungicidas, localizado na pasta do drive.\n\ndat <- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/15pCj0zljvd-TGECe67OMt6sa21xO8BqUgv4d-kU8qi8/edit#gid=0\")\n\nTrabalhando os dados:\n\noptions(scipen=999)\ndat2 <- dat |> \n  dplyr::select(-Isolate, Population) |> \n  group_by(Code, Year, Dose) |> \n  summarise(GC_mean = mean(GC))\n\nO comando options(scipen=999) define a opção scipen para o valor 999. A opção scipen é usada para controlar a exibição de números científicos em notação exponencial. Definir scipen como 999 desativa a notação exponencial e exibirá os números completos.\nEsse bloco de comandos realiza uma sequência de operações no dataframe dat: Remove as colunas “Isolate” e “Population” (a coluna “Isolate” e a coluna “Population” não estarão presentes no resultado dat2). Agrupa os dados restantes por “Code”, “Year” e “Dose”. Calcula a média da coluna “GC” para cada grupo e armazena os resultados na coluna “GC_mean” em um novo dataframe chamado date, ou seja, summarise(GC_mean = mean(GC)) está sendo utilizado para calcular a média da coluna “GC” dentro de cada grupo definido pelas colunas “Code”, “Year” e “Dose”. A média resultante é atribuída a uma nova coluna chamada “GC_mean” no dataframe resultante.\nPara fazer um gráfico só com um dos isolados:\n\nFGT152 <- dat2 |>\n  filter(Code == \"FGT152\")\n\nFGT152 |>\n  ggplot(aes(factor(Dose), GC_mean))+\n  geom_point()+\n  geom_line()+\n  facet_wrap(~ Code)\n\n\n\n\nCriou-se um gráfico usando o dataframe FGT152, que é um subconjunto dos dados filtrados do dat2. O gráfico mostra os valores de “GC_mean” em relação a “Dose” para o código “FGT152”, com pontos e uma linha contínua. Os painéis separados são criados para cada nível único de “Code”, mas neste caso, como só há “FGT152”, há apenas um painel.\n\n\nO pacote DRC é especialmente útil para ajustar modelos de regressão dose-resposta e realizar análises estatísticas relacionadas. Ele oferece modelos para ajustar curvas de dose-resposta, incluindo modelos log-logísticos, log-probit, Weibull, etc. O pacote fornece funções para calcular estimativas de EC50.\n\n\nModelo log.logistico3: o modelo de três parâmetros log-logístico descreve a relação entre a dose de um agente ou tratamento e a resposta biológica. Esse modelo assume uma resposta crescente ou decrescente com a dose e é frequentemente usado quando se espera uma resposta em forma de curva em S ou uma resposta em forma de S invertido.\n\nlibrary(drc)\n\ndrc1 <- drm(GC_mean ~ Dose, data = FGT152,\n            fct = LL.3())\nAIC(drc1)\n\n[1] 33.60846\n\nsummary(drc1)\n\n\nModel fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)\n\nParameter estimates:\n\n               Estimate Std. Error t-value     p-value    \nb:(Intercept)  0.401905   0.053427  7.5225    0.001672 ** \nd:(Intercept) 47.540342   1.459890 32.5643 0.000005302 ***\ne:(Intercept)  7.220130   2.340119  3.0854    0.036739 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error:\n\n 1.993805 (4 degrees of freedom)\n\nplot(drc1)\n\n\n\nED(drc1, 50)\n\n\nEstimated effective doses\n\n       Estimate Std. Error\ne:1:50   7.2201     2.3401\n\n\nO comando drc1 <- drm(GC_mean ~ Dose, data = FGT152, fct = LL.3()) : esses comandos são utilizados para ajustar um modelo de regressão de dose-resposta usando a função drm() da biblioteca ‘drc’ (dose-response modeling). O argumento GC_mean ~ Dose especifica que a variável resposta é “GC_mean” e a variável preditora é “Dose”. O argumento data = FGT152 indica que os dados para ajuste do modelo são provenientes do dataframe FGT152. O argumento fct = LL.3() especifica o tipo de modelo a ser ajustado, neste caso, um modelo de três parâmetros log-logístico (LL.3()). AIC(drc1) é a função que utilizamos para calcular o AIC para o modelo ajustado. O AIC é uma medida que avalia a qualidade relativa do modelo, considerando a qualidade do ajuste e a complexidade do modelo. Quanto menor o valor do AIC, melhor o ajuste. A função ED(drc1, 50): calcula a dose efetiva (ED) para um determinado nível de resposta, neste caso, 50%. O comando ED() é usado para calcular as doses efetivas para um modelo de regressão de dose-resposta. O argumento drc1 especifica o modelo ajustado e o argumento 50 indica o nível de resposta desejado (50% neste caso).\nModelo W1.3: tem melhor ajuste atraves do AIC. Esse modelo permite uma maior flexibilidade na modelagem da forma da curva de resposta em relação à dose, pois inclui o parâmetro de assimetria (g). Esse parâmetro permite que a curva de resposta seja assimétrica, podendo assumir formas como curvas em forma de S assimétricas.\n\ndrc1 <- drm(GC_mean ~ Dose, data = FGT152,\n            fct = W1.3())\nAIC(drc1)\n\n[1] 37.75192\n\nsummary(drc1)\n\n\nModel fitted: Weibull (type 1) with lower limit at 0 (3 parms)\n\nParameter estimates:\n\n              Estimate Std. Error t-value    p-value    \nb:(Intercept)  0.28354    0.04760  5.9567   0.003987 ** \nd:(Intercept) 48.38112    2.09996 23.0390 0.00002103 ***\ne:(Intercept) 30.12379   12.58003  2.3946   0.074796 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error:\n\n 2.680509 (4 degrees of freedom)\n\nplot(drc1)\n\n\n\nED(drc1, 50)\n\n\nEstimated effective doses\n\n       Estimate Std. Error\ne:1:50   8.2704     3.6719\n\n\n\n\n\n\nFornece os valores de EC50 para os dados. É útil para comparar as estimativas de EC50 entre diferentes identificadores (ID) e identificar possíveis diferenças na resposta à dose.\n\nlibrary(ec50estimator)\n\ndf_ec50 <- estimate_EC50(GC_mean ~ Dose,\n                         data = dat2,\n                         isolate_col = \"Code\",\n                         interval = \"delta\",\n                         fct = drc::LL.3())\n\ndf_ec50 |>\n  ggplot(aes(Estimate, reorder(ID, Estimate)))+\n  geom_point()+\n  geom_errorbar(aes(xmin = Lower,\n                    xmax = Upper), width = 0.1)+\n  xlim(0,30)\n\n\n\n\nA função estimate_EC50() é utilizada para estimar os valores de EC50 a partir dos dados. Dentro dessa função são usados varios argumentos, esses argumentos possuem diferentes papeis na função: O isolate_col = “Code” foi usado para definir a coluna “Code” como uma coluna de identificação única para diferentes amostras. O interval = “delta” define o tipo de intervalo de confiança que deverá ser calculado para as estimativas de EC50. o comando fct = drc::LL.3() especifica o modelo de ajuste, neste caso, o modelo de três parâmetros log-logístico. Dentro do ggplot e da função aes o argumento (Estimate, reorder(ID, Estimate)) especifica as variáveis a serem mapeadas nos eixos x e y do gráfico. “Estimate” corresponde aos valores estimados de EC50 e “ID” é reordenado de acordo com os valores de Estimate para controlar a ordem no gráfico. Nesse caso, a função geom_errorbar adiciona barras de erro ao gráfico, utilizando os valores inferiores (Lower) e superiores (Upper) dos intervalos de confiança das estimativas de EC50. O xlim(0,30) define os limites do eixo x do gráfico, limitando o intervalo de visualização das estimativas até 30."
  },
  {
    "objectID": "Aula18.html",
    "href": "Aula18.html",
    "title": "Transformação de dados Box-Cox",
    "section": "",
    "text": "Transformação de variável tipo boxcox\nA transformação de variável tipo Box-Cox é uma técnica utilizada na análise estatística para melhorar a adequação dos dados a pressupostos de normalidade e homogeneidade de variância. Essa transformação é aplicada a variáveis contínuas positivas que possuem assimetria ou heterogeneidade de variância. A transformação de Box-Cox é definida pela seguinte equação: y(lambda) = (x^lambda - 1) / lambda\nNessa equação, “x” representa a variável original, “y(lambda)” representa a variável transformada para um determinado valor de lambda e “lambda” é o parâmetro de transformação que varia de -∞ a +∞. O valor de lambda determina o tipo de transformação aplicada: Se lambda = 0, a transformação de Box-Cox é equivalente ao logaritmo natural (ln). Se lambda = 1, a transformação de Box-Cox é equivalente à transformação linear (sem transformação). Se lambda < 0, é aplicada uma transformação inversa.\nPreparo pré-análise Para reakizar essse tipo de transformação, usa-se o pacote MASS.\n\nlibrary(MASS)\n\nDados para exemplificação: InsectSprays, do próprio R. A função boxcox() pode ser utilizada para calcular a transformação de Box-Cox e identificar o valor de lambda ótimo para uma determinada variável. Essa função retorna uma lista de resultados, incluindo o valor de lambda ótimo e gráficos de diagnóstico.\n\ninsects <- InsectSprays\n\nb <- boxcox(lm(insects$count+0.1 ~1))\n\n\n\nlambda <- b$x[which.max(b$y)]\nlambda\n\n[1] 0.4242424\n\ninsects$count2 <-(insects$count ^ lambda - 1) / lambda\nhist(insects$count)\n\n\n\nhist(insects$count2)\n\n\n\ninsects$count2\n\n [1]  3.903635  3.024469  6.043993  4.864268  4.864268  4.407118  3.903635\n [8]  6.557185  5.484274  6.043993  4.864268  4.640760  4.161975  5.484274\n[15]  6.219699  4.161975  5.285168  4.864268  5.484274  5.484274  5.863153\n[22]  6.219699  3.024469  4.640760 -2.357143  0.000000  3.024469  0.805831\n[29]  1.399509  0.000000  0.805831  0.000000  1.399509 -2.357143  0.000000\n[36]  1.887150  1.399509  2.308577  4.407118  2.683787  1.887150  1.399509\n[43]  2.308577  2.308577  2.308577  2.308577  0.805831  1.887150  1.399509\n[50]  2.308577  1.399509  2.308577  1.399509  2.683787  0.000000  0.000000\n[57]  1.399509  0.805831  2.683787  1.887150  4.161975  3.629951  5.078760\n[64]  6.390651  5.078760  5.285168  4.640760  3.903635  7.033117  7.033117\n[71]  6.719601  4.640760"
  },
  {
    "objectID": "Aula2.html",
    "href": "Aula2.html",
    "title": "Pacotes no R",
    "section": "",
    "text": "Um pacote é como uma coleção de funções, dados e arquivos de ajuda agrupados em uma estrutura padrão bem definida que pode ser baixada e instalada em R. Então, pode-se dizer que os pacotes são códigos uteis e instaláveis que permitem estender os recursos do R para realizar as funções desejadas. Esses pacotes são compartilhados e podem ser baixados de várias fontes, sendo as mais populares o CRAN, Bioconductor e o GitHub. CRAN – é o repositório oficial do R para pacotes montados por usuários. Bioconductor – fornece software de código aberto orientado para bioinformática. GitHub – é um site que hospeda repositórios git para todos os tipos de software e projetos (não apenas R). É onde as versões de desenvolvimento de ponta dos pacotes R são hospedadas\nOs pacotes necessários para rodar análises no R depende do objetivo do trabalho a ser realizado, mas alguns pacotes são básicos e essenciais para a maioria das análises, como: Tidyverse, dplyr, readxl, ggplot, etc. Nas próximas aulas, aprofundaremos um pouco mais sobre esses pacotes.\n\n\nA instalação e o carregamento de pacotes pode ser feito pelo menu ou então com um comando no console. O fluxo básico pode ser conferido neste link. A instalação é feita através do menu: tools > install packages, indo em packages > install e digitando o nome do pacote para baixar ou usando a função instal.packages(nome do pacote) no console. Para o carregamento de pacotes, a função utilizada é library(nome do pacote). O carregamento pode ser feito tanto pelo library, anotando entre parênteses o nome do pacote, quanto separando os programas por vírgula dentro do mesmo parêntese. Muitas vezes, durante o carregamento de pacotes, usamos as hashtags (#) seguidas por / warning: false e / message: false para desativar a exibição de avisos (warnings) e mensagens durante a execução do código, já que muitos pacotes exibem mensagens e avisos grandes após o carregamento que poluem o arquivo.\n\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(gsheet)\nlibrary(remotes)\n\n\n\n\nAs funções de pacotes são variadas e serão apresentadas de forma mais aprofundada na próxima aula (Dataframe). Enquanto isso, veremos rapidamente como invocar uma função de um pacote, mais especificamente a função arrange, do pacote dplyr. Para isso, utilizaremos o conjunto de dados mtcars, um conjunto de dados incorporado no R. “arrange()” é uma função do pacote dplyr que permite ordenar os dados com base em uma ou mais variáveis. Nesse caso, a função “arrange()” está sendo aplicada ao conjunto de dados “mtcars”, e a variável “cyl” está sendo especificada como a variável de ordenação. Isso significa que os dados serão reorganizados de forma ascendente com base na variável “cyl”). Nas linhas seguintes, os comandos utilizados referem-se a busca de uma planilha especifica do google sheet e a instalação de um pacote “r4pde” diretamente do repositório GitHub do uduário Emerson Del Ponte. O sinal de igual é utilizado para atribuição, que pode ser substituido por <-. A função head() retorna as primeiras linhas de um objeto de dados ou de um conjunto de dados, nesse caso, foi utilizado para retornar as primeiras linhas do conjunto mtcars.\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\narrange(mtcars, cyl)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\nOrange\n\nGrouped Data: circumference ~ age | Tree\n   Tree  age circumference\n1     1  118            30\n2     1  484            58\n3     1  664            87\n4     1 1004           115\n5     1 1231           120\n6     1 1372           142\n7     1 1582           145\n8     2  118            33\n9     2  484            69\n10    2  664           111\n11    2 1004           156\n12    2 1231           172\n13    2 1372           203\n14    2 1582           203\n15    3  118            30\n16    3  484            51\n17    3  664            75\n18    3 1004           108\n19    3 1231           115\n20    3 1372           139\n21    3 1582           140\n22    4  118            32\n23    4  484            62\n24    4  664           112\n25    4 1004           167\n26    4 1231           179\n27    4 1372           209\n28    4 1582           214\n29    5  118            30\n30    5  484            49\n31    5  664            81\n32    5 1004           125\n33    5 1231           142\n34    5 1372           174\n35    5 1582           177\n\nurl = gsheet2tbl('docs.google.com/spreadsheets/d/1I9mJsS5QnXF2TNNntTy-HrcdHmIF9wJ8ONYvEJTXSNo') \n\nb = url\ninstall_github(\"emdelponte/r4pde\")\n\nvctrs  (0.5.2 -> 0.6.3) [CRAN]\ncli    (3.6.0 -> 3.6.1) [CRAN]\ntibble (3.1.8 -> 3.2.1) [CRAN]\ndplyr  (1.1.0 -> 1.1.2) [CRAN]\nigraph (1.4.3 -> 1.5.0) [CRAN]\n\n  There is a binary version available but the source version is later:\n       binary source needs_compilation\nigraph  1.4.3  1.5.0              TRUE\n\npackage 'vctrs' successfully unpacked and MD5 sums checked\npackage 'cli' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\nivia\\AppData\\Local\\Temp\\RtmpmaWy57\\downloaded_packages\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file 'C:\\Users\\nivia\\AppData\\Local\\Temp\\RtmpmaWy57\\remotes3bfc5dd13bbb\\emdelponte-r4pde-c1cc0b7/DESCRIPTION' ... OK\n* preparing 'r4pde':\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building 'r4pde_0.0.0.9000.tar.gz'\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nPacote r4pde: Em linhas gerais, a variável “unit” é criada e atribuída com os valores de 1 a 12 através da função c(1:12), depois a variável “class” também é criada. Posteriormente, é feita a criação de um data frame “ratings” com essas variáveis e, em seguida, aplica a função “DSI” do pacote r4pde aos dados. O bloco de comandos mais abaixo fornece instruções para visualizar os valores, calcular a média (mean), o desvio padrão (sd) e obter um resumo estatístico da variável “class” do conjunto de dados “ratings” (função summary).\n\nlibrary(r4pde)\nunit <- c(1:12)\nclass <- c(2,3,1,1,3,4,5,0,2,5,2,1)\nratings <- data.frame(unit, class)\nDSI(unit = ratings$unit, class = ratings$class, max = 6)\n\n[1] 40.27778\n\nratings$class\n\n [1] 2 3 1 1 3 4 5 0 2 5 2 1\n\nmean(ratings$class)\n\n[1] 2.416667\n\nsd(ratings$class)\n\n[1] 1.621354\n\nsummary(ratings$class)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.000   2.000   2.417   3.250   5.000 \n\nsummary(class)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.000   2.000   2.417   3.250   5.000"
  },
  {
    "objectID": "Aula3.html",
    "href": "Aula3.html",
    "title": "Dataframe",
    "section": "",
    "text": "A base de dados é representada por objetos chamados de data frames. A principal característica de um data frame é possuir linhas e colunas. Para a leitura de dados em R, deve-se salvar o arquivo em um formato adequado.\n\n\nComo mencionado na aula anterior (Pacotes e carregamento de pacotes), existem alguns pacotes que são essenciais para realizar análises no R e, tratando-se de data frames, alguns deles se destacam.\nPara carregar o dataframe, pode-se usar pacotes do R, a exemplo do pacote dataset, que tem em suas funções o pacote cars.\n\ncars\n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n11    11   28\n12    12   14\n13    12   20\n14    12   24\n15    12   28\n16    13   26\n17    13   34\n18    13   34\n19    13   46\n20    14   26\n21    14   36\n22    14   60\n23    14   80\n24    15   20\n25    15   26\n26    15   54\n27    16   32\n28    16   40\n29    17   32\n30    17   40\n31    17   50\n32    18   42\n33    18   56\n34    18   76\n35    18   84\n36    19   36\n37    19   46\n38    19   68\n39    20   32\n40    20   48\n41    20   52\n42    20   56\n43    20   64\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n\ncars2 <- cars\nspeed <- cars2$speed\nspeed\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\n\nPara criar um dataframe de pacote, basta carregar o pacote e depois atribui-lo. Como já mencionado, para atribuir uma função a outra, basta usar <-. Ex.: buscar em packages o pacote r4pde e carrega-lo (usando a função library), dentro do quadro de ajuda deste pacote, é possível encontrar os dataframes incorporados, como o RustSoybean. Para usar esse quadro de dados basta usar a função de atribuição.\n\nlibrary(r4pde)\ndf <- RustSoybean\ndf\n\n# A tibble: 34 × 7\n   epidemia latitude longitude local              planting   detection  severity\n      <dbl>    <dbl>     <dbl> <chr>              <date>     <date>        <dbl>\n 1       23    -23.0     -50.1 Cambara            2003-11-25 2004-02-02     24  \n 2       24    -24.0     -52.4 Campo Mourao       2003-11-28 2004-02-02     21  \n 3       31    -15.5     -55.2 Campo Verde        2004-11-20 2005-01-25     78  \n 4        3    -13.3     -44.6 Correntina         2002-11-10 2003-01-03     85  \n 5       15    -13.3     -44.6 Correntina         2003-11-28 2004-01-31     25  \n 6       34    -25.4     -51.5 Guarapuava         2004-11-29 2005-03-14     32  \n 7        7    -29.2     -53.7 Julio Castilhos    2002-12-15 2003-04-10     40  \n 8       13    -12.1     -45.8 Luis Eduardo Maga… 2003-11-12 2004-02-15     39.2\n 9       33    -12.1     -45.8 Luis Eduardo Maga… 2004-11-19 2005-01-25     55  \n10        1    -23.3     -51.2 Londrina           2002-11-06 2003-02-03     45  \n# ℹ 24 more rows\n\n\nA função para a leitura de dados no R é a read.table. O R possui um bom número de variantes desta função, usadas para importar uma variedade de formatos de arquivos diferentes no R. As variações da função read.table mais úteis são as funções read.csv e read.csv2 e readxl.\nTidyverse: é um pacote que consolida uma série de ferramentas. Fazem parte do tidyverse os pacotes dplyr, tidyr, readr, ggplot2, entre muitos outros.\n\n\n\n\nPara importar dados diretamente de outros softwares, é necessário instalar pacotes, a exemplo do readxl, usado para a importação de planilhas do excel. O pacote readxl permite gerar dataframe de planilhas excel e é a forma mais simples de carregar um arquivo neste formato. O pacote readxl não é do conjunto tidyverse, então precisa ser carregado sempre antes de usar. A maioria das funções do readxl permite ler dados de planilhas excel, a exemplo de read_excel. O primeiro argumento para read-excel é o caminho do arquivo que deve ser lido. O caminho do arquivo e o nome do arquivo (incluindo a extensão do arquivo) precisam ser colocados entre aspas duplas, pois a função espera que seja uma função de caractere.\n\nlibrary(readxl)\nmagnesio <- read_excel(\"dados-diversos.xlsx\")\nescala1 <- read_excel(\"dados-diversos.xlsx\", \"escala\")\nescala2 <- read_excel(\"dados-diversos.xlsx\", 2)\n\nNo Excel, um arquivo pode ter várias planilhas. Por padrão, as funções de leitura trarão apenas a primeira planilha do arquivo. Para trazer outra planilha específica, basta utilizarmos o argumento sheet+o número da planilha (linha 3 do comando acima), ou adicionar o nome da planilha desejada entre aspas duplas, separada do nome da planilha geral por uma vírgula (linha 2 do comando acima).\n\n\nO pacote readr do tidyverse é utilizado para importar arquivos de texto, como .txt ou .csv, para o R e seu carregamento é feito, assim como os demais, pela função library. O readr transforma arquivos de textos em tibbles usando as funções como: read.csv() ou read_csv() - usada para importar arquivos de valores separados por , e com a primeira linha dos dados com os nomes das variáveis. read.csv2 ou read_csv2() - usada para importar bases de arquivos separados por ponto-e-vírgula no R. Em alguns países, como o Brasil, as vírgulas são utilizadas para separar as casas decimais dos números, inviabilizando o uso de arquivos .csv. Nesses casos, quando a vírgula é o separador de decimal, os arquivos .csv passam a ser separados por ponto-e-vírgula, assim, esta função assume que os dados são separados por ; e que uma vírgula é usada ao inves de um ponto decimal.\nCSV:\n\nlibrary(tidyverse)\n\n#using read.csv\n\nmagnesio2 <- read.csv(\"dados-diversos.csv\")\n\n#using read_csv\nmagnesio3 <- read_csv(\"dados-diversos.csv\")\nmagnesio3\n\n# A tibble: 60 × 4\n   Irrigation   rep   day severity\n   <chr>      <dbl> <dbl>    <dbl>\n 1 Furrow         1     0     0.01\n 2 Furrow         2     0     0.01\n 3 Furrow         3     0     0.01\n 4 Furrow         1     7     0.04\n 5 Furrow         2     7     0.04\n 6 Furrow         3     7     0.04\n 7 Furrow         1    14     0.1 \n 8 Furrow         2    14     0.1 \n 9 Furrow         3    14     0.11\n10 Furrow         1    21     0.11\n# ℹ 50 more rows\n\n\nTXT: As funções do reader são usadas de forma semelhante as do read.table, sendo que muitas vezes usam os mesmos argumentos. Arquivos em formato txt podem ser importados usando o read.table.\n\n# using read.table\n\nmagnesio4 <-read.table(\"dados-diversos.txt\", header = TRUE)\nmagnesio4\n\n   Irrigation.rep.day.severity\n1              Furrow,1,0,0.01\n2              Furrow,2,0,0.01\n3              Furrow,3,0,0.01\n4              Furrow,1,7,0.04\n5              Furrow,2,7,0.04\n6              Furrow,3,7,0.04\n7             Furrow,1,14,0.10\n8             Furrow,2,14,0.10\n9             Furrow,3,14,0.11\n10            Furrow,1,21,0.11\n11            Furrow,2,21,0.10\n12            Furrow,3,21,0.10\n13            Furrow,1,28,0.15\n14            Furrow,2,28,0.17\n15            Furrow,3,28,0.15\n16            Furrow,1,35,0.18\n17            Furrow,2,35,0.19\n18            Furrow,3,35,0.19\n19            Furrow,1,42,0.34\n20            Furrow,2,42,0.38\n21            Furrow,3,42,0.34\n22            Furrow,1,49,0.38\n23            Furrow,2,49,0.39\n24            Furrow,3,49,0.38\n25            Furrow,1,56,0.40\n26            Furrow,2,56,0.41\n27            Furrow,3,56,0.43\n28            Furrow,1,63,0.46\n29            Furrow,2,63,0.46\n30            Furrow,3,63,0.43\n31               Drip,1,0,0.01\n32               Drip,2,0,0.01\n33               Drip,3,0,0.01\n34               Drip,1,7,0.03\n35               Drip,2,7,0.04\n36               Drip,3,7,0.04\n37              Drip,1,14,0.11\n38              Drip,2,14,0.11\n39              Drip,3,14,0.10\n40              Drip,1,21,0.13\n41              Drip,2,21,0.12\n42              Drip,3,21,0.10\n43              Drip,1,28,0.16\n44              Drip,2,28,0.15\n45              Drip,3,28,0.15\n46              Drip,1,35,0.18\n47              Drip,2,35,0.19\n48              Drip,3,35,0.17\n49              Drip,1,42,0.30\n50              Drip,2,42,0.33\n51              Drip,3,42,0.34\n52              Drip,1,49,0.33\n53              Drip,2,49,0.37\n54              Drip,3,49,0.37\n55              Drip,1,56,0.39\n56              Drip,2,56,0.46\n57              Drip,3,56,0.41\n58              Drip,1,63,0.43\n59              Drip,2,63,0.43\n60              Drip,3,63,0.43\n\n\nObs.: O argumento header = TRUE especifica que a primeira linha dos dados contem o nome das variáveis (ex. magnesio, nitrogenio), se este não for o caso é só usar o argumento header = FALSE.\n\n\n\n\nPara importar planilhas google, usa a função gsheet (read_sheet), presente no pacote googlesheets4 ou o pacote gsheet. Como esses pacotes também não fazem parte do conjunto tidyverse, precisa carregá-los para usar. A função read_gsheet lê o arquivo (planilha google) a partir de uma URL (copia e cola o link da planilha desejada).\n\n# using gsheet package\n\nlibrary(gsheet)\nmagnesio5 <- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1aID5Dh6PlBVCKzU1j7k-WA6zuWQWE2NhtWEgdJtt5iA/edit?usp=sharing\")\n\nsurvey <- gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1aID5Dh6PlBVCKzU1j7k-WA6zuWQWE2NhtWEgdJtt5iA/edit#gid=366054269\")\n\n# no caso de ter um link da internet\n\nfusarium <- read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/main/data/fusarium_banana.csv\")\nfusarium\n\n# A tibble: 163 × 4\n     lon   lat marker  field\n   <dbl> <dbl> <chr>   <dbl>\n 1 -50.8 -20.5 plantas    11\n 2 -50.8 -20.5 plantas    11\n 3 -50.8 -20.5 plantas    11\n 4 -50.8 -20.5 plantas    11\n 5 -50.8 -20.5 plantas    11\n 6 -50.8 -20.5 plantas    11\n 7 -50.8 -20.5 plantas    11\n 8 -50.8 -20.5 plantas    11\n 9 -50.8 -20.5 plantas    11\n10 -50.8 -20.5 plantas    11\n# ℹ 153 more rows"
  },
  {
    "objectID": "Aula4.html",
    "href": "Aula4.html",
    "title": "Visualização de dados",
    "section": "",
    "text": "O ggplot2 é o pacote usado para visualização dos dados. O pacote ggplot2 pode ser carregado individualmente, ou pode ser carregado pelo tidyverse.Para visualizar os dados, basta usar a função ggplot.\n\n\n\n\n\n\n\nApós carregar o pacote, importa-se um banco de dados para trabalho. Para o formato csv, é necessário carregar o pacote tidyverse. As variáveis podem ser categóricas ou numéricas. As variáveis categóricas são entendidas no R como caracteres.\n\nlibrary(tidyverse)\nmg <- read_csv(\"dados-diversos.csv\")\n\n\n\n\nA programação de gráficos em ggplot2 é feito na forma de camadas, que são adicionadas à medida que se confecciona o gráfico, por isso que se usa o sinal de +, porque significa a adição de mais uma camada. Usa-se o pipe (|>) para enfatizar uma sequência de comandos ou ações no chunk e para evitar adicionar o nome do data frame dentro da função ggplot.O pipe deve ter sempre um espaço antes dele e, geralmente, deve ser seguido por uma nova linha. Após a primeira etapa, cada linha deve ter dois espaços, o que torna mais fácil adicionar outras etapas ou reorganizar as já existentes.\n\nlibrary(ggplot2)\n\nggplot: A primeira função para criar um ggplot é a função ggplot, que define o conjunto de dados e elementos de estética dos gráficos. O primeiro elemento ou argumento da função é o conjunto de dados a ser utilizado (o data frame onde nossos dados foram armazenados, ex. mg). A função ggplot() define o plano com os eixos x e y. aes: Após isso, precisamos especificar como as observações serão mapeadas nos aspectos visuais do gráfico e quais formas geométricas serão utilizadas. Para isso, usamos a função aestetic (aes), uma das primeiras funções em ggplot, usada para descrever como as variáveis são mapeadas (eixos x e y). Para adionar cor usa-se a função color = nome da cor. Ao incluir esta função dentro da função aes, dizemos ao ggplot que os pontos devem ser mapeados esteticamente utilizando cores para cada variavel (ex.irrigação). A função geo_point define que a forma geométrica a ser utilizada é baseada em pontos, gerando um gráfico de dispersão. A função alpha trabalha com a tansparência.\n\nmg |>\n  ggplot(aes(Irrigation, severity, color = Irrigation))+\n  geom_point(alpha = 0.5)\n\n\n\n\nA função shape é utilizada para adicionar diferentes formas ou tipos de marcadores para diferenciar as variáveis (ex. quadrado e triângulo).\n\nmg |>\n  ggplot(aes(Irrigation, severity, shape = Irrigation))+\n  geom_point(alpha = 0.5)\n\n\n\n\nAgora, mudamos a váriavel irrigation por day. Para isso, basta subtituir a variável dentro da função aes. A função filtro, como o próprio nome remete, filtra determinadas linhas e a função geom_line transforma o gráfico em um gráfico de linhas:\n\n# mudando para a variável day\nmg |> \n  ggplot(aes(day, severity, shape = Irrigation))+\n  geom_point(alpha = 0.5)\n\n\n\n# Deixando apenas 1 repetição - função filter e alterando para gráfico de linhas\nmg |> \n  filter(rep == 1) |>\n  ggplot(aes(day, severity, shape = Irrigation))+\n  geom_point(alpha = 0.5)+geom_line()\n\n\n\n\nA função facet_wrap adiciona facetas. Ou seja, com essa função informamos que um gráfico deve ser criado para cada ambiente, um grafico diferente para cada variável.\n\n# Dividindo o número de repetições - função facet_wrap\nmg |> \n  ggplot(aes(day, severity, shape = Irrigation))+\n  geom_point(alpha = 0.5)+\n  geom_line()+\n  facet_wrap(~rep)\n\n\n\n\nOutras funções:\n\n#Função select - seleciona colunas\nmg |> \nselect(Irrigation, severity) |>\nggplot(aes(Irrigation, severity, shape = Irrigation))+geom_point(alpha = 0.5)\n\n\n\n# Retirando group e geom_boxplot\nmg |> \n  select(day, severity) |>\n  ggplot(aes(day, severity))+\n  geom_point()\n\n\n\n# Agrupando por repetição: As primeiras 3 linhas separam a severidade por dia.\nmg |> \n  select(day, rep, severity) |>\n  group_by(day) |>\n  summarise(sev = mean(severity)) |>\n  ggplot(aes(day, sev))+\n  geom_point()\n\n\n\n\n\nmg |> \n  select(Irrigation, severity) |>\n  ggplot(aes(Irrigation, severity, shape = Irrigation))+geom_boxplot()+\n  geom_point(alpha = 0.5)\n\n\n\n# Mudando a variável irrigação por day\nmg |> \n  select(day, severity) |>\n  ggplot(aes(day, severity))+\n  geom_boxplot()+\n  geom_point(alpha = 0.5)\n\n\n\n# Separando por grupo - função group\nmg |> \n  select(day, severity) |>\n  ggplot(aes(day, severity, group = day))+\n  geom_boxplot()+\n  geom_point()\n\n\n\n\nCriando um novo conjunto, fazendo a media e adicionando titulo no gráfico:\n\nmg2 <- mg  |> \n  select(day, rep, severity) |>\n  group_by(day) |>\n  summarise(sev = mean(severity)) |>\n  ggplot(aes(day, sev))+\n  geom_point()"
  },
  {
    "objectID": "Aula5.html",
    "href": "Aula5.html",
    "title": "Tipos de gráficos",
    "section": "",
    "text": "Existe uma grande diversidade de gráficos que podem ser utilizados para representar dados. Os de uso mais comum em nossas aulas são: boxplots, scatterplots, histogramas, gráficos de linha e gráficos de barra e colunas.\n\n\nBoxplots são úteis para estudarmos a distribuição de uma variável, principalmente quando queremos comparar várias distribuições. São frequentemente usados para representar dados numéricos, como medidas de tendência central, variabilidade e distribuição. Eles mostram a distribuição de um conjunto de dados por meio de um diagrama que inclui a mediana, quartis, outliers e limites do conjunto de dados. Os boxplots ajudam a visualizar a distribuição de um conjunto de dados e identificar possíveis outliers ou pontos de dados incomuns. Para construir um boxplot no ggplot, utilizamos a função geom_boxplot. Ele precisa dos atributos x e y, sendo que ao atributo x devemos mapear uma variável categórica.\nA função geom_boxplot compreende as seguintes estéticas: • outlier.color = NA: Retira a cor de fungo do boxplot; • fill = define a cor. O nome da cor, por ser texto, deve ser escrita entre aspas; • size = (número): define o tamanho do box, retorna um vetor numérico de comprimento length(x ).\nImportação de dados: sempre carregar primeiro o tidyverse e depois o pacote que irá usar para importar os dados, no caso de planilhas excel, o pacote readxl. Após isso, deve escolher a atribuição dos dados.\n\nlibrary(tidyverse)\nlibrary(readxl)\nmg <- read_excel(\"dados-diversos.xlsx\")\n\nVisualização dos dados: como abordado na aula aunterior, o ggplot é o pacote usado para visualização dos dados. Para visualizar os dados, basta usar a função ggplot.É importante que se defina antes quem será o eixo x e y (no caso, y é o comprimento da lesão, enquanto x são os tratamentos - controle e mg2). Depois, usa a função aes (aesthetic) do que deseja que apareça no gráfico e adiciona a camada geom_point para que os dados sejam vistos em forma de pontos.\n\nmg |> \n  ggplot(aes(trat, comp))+\n  geom_point()\n\n\n\n\nMuitas vezes, ocorre a sobreposição de pontos no gráfico. Para desagregar esses pontos, usa-se a função geo_jitter(). Esta função adiciona variação aleatória aos pontos de localização do gráfico. Em outras palavras, ele “agita” ligeiramente as localizações dos pontos. Este método reduz o overplotting, pois é improvável que dois pontos com a mesma localização tenham a mesma variação aleatória. Para evitar que os dados fiquem muito dispersos, pode-se definir a largura (Width - controla a quantidade de deslocamento vertical).\n\nmg |> \n  ggplot(aes(trat, comp))+\n  geom_jitter(width = 0.1)\n\n\n\n\nPara adicionar o boxplot e mudar a visão do gráfico, basta adicionar mais uma camada, ou seja, acionar a função geom_boxplot.\n\nmg |> \n  ggplot(aes(trat, comp))+\n  geom_boxplot()+\n  geom_jitter(width = 0.1)\n\n\n\n\nPara tirar a cor do outlier, aumentar o tamanho dos pontos e colorir o boxplot:\n\np_box <- mg |> \n  ggplot(aes(trat, comp))+\n  geom_boxplot(outlier.color = NA,\n               fill = \"orange\",\n               size = 0.5)+\n  geom_jitter(width = 0.1,\n              height = 0,\n              color = \"black\")+\n  scale_y_continuous(limits = c(5,20), \n                     n.breaks = 10)+\n  labs(y = \"Lesion size (mm)\",\n       x = \" \")+\n  theme_bw()\np_box\n\n\n\nggsave(\"figs/plot2.png\", bg = \"white\")\n\nPara sumarizar os dados, usa-se a summarise e função media da variável comprimento. Depois disso, transforma no ggplot e muda a estética. Para adicionar a barra de erro, usa-se a função errorbar. Para omitir os comandos das colunas, basta adicionar uma # na frente da linha de comando.\n\np_means <- mg |>\n  group_by(trat) |>\n  summarise(comp_mean = mean(comp),\n            comp_sd = sd(comp)) |>\n  ggplot(aes(trat, comp_mean))+\n  #geom_col(fill = \"orange\",\n           #width = 0.5)+\n  geom_point()+\n  scale_y_continuous(limits = c(7,18), \n                     n.breaks = 6)+\n  geom_errorbar(aes(ymin = comp_mean - comp_sd, \n                    ymax = comp_mean + comp_sd,\n                    width = 0.05))+\n  theme_bw()+\n  labs(y = \"Lesion size (mm)\",\n       x = \" \")\np_means\n\n\n\n#salvando gráfico\nggsave(\"figs/mean_sd.png\", \n       width = 4,\n       bg = \"white\")\n\nInstalando o ggthemes e carregando pacote:\n\nlibrary(ggthemes)\n\n\n\nO patchwork torna possível combinar ggplots diferentes e separados em um mesmo gráfico. Depois de instalar, carrega o pacote. Primeiramente, deve-se definir um nome para cada gráfico. Os nomes devem ser dados dentro do chunk de cada gráfico. O nome deve ser escolhido e o banco de dados estudado deve ser atribuído a ele, tendo logo em seguida a necessidade de inserção de um pipe. Então, para unir diferentes ggplots, é necessário utilizar o título desses ggplots separados pelo sinal de + ou pela barra reta (|).\n\nlibrary(patchwork)\np_box + p_means\n\n\n\n#ou então usar a função abaixo\np_box | p_means\n\n\n\n# para adicionar letras A, B para identificar o grafico e adicionar titulo:\n(p_box | p_means)+\n  plot_annotation(tag_levels = \"A\",\n                  title = 'Gráficos que impressionam')\n\n\n\nggsave(\"figs/combined.png\")\n\n\n\n\n\nOs gráficos de colunas são usados para representar dados categóricos ou discretos (que podem ser divididos em grupos ou categorias). Um gráfico de colunas pode ajudar a identificar as tendências e padrões nos dados e destacar diferenças significativas entre as categorias. Para transformar gráficos em gráficos de colunas, usa-se a função geom_col (). Para esse tipo de gráficos, vamos utilizar o conjunto de dados survey. Para inverter as barras usa-se a função coord_flip().\n\nsurvey <- read_excel(\"dados-diversos.xlsx\",\n                     sheet = \"survey\")\nsurvey |> \n  filter(state == \"RS\") |>\n  #Para contar o numero de ocorrencias em cada classe:\n  count(species) |>\n  ggplot(aes(species, n))+\n  geom_col(width = 0.4,\n           fill = \"steelblue\")+\n  coord_flip()+\n  labs(x = \" \", y = \"Number of isolates\",\n       tiltle = \"Horizontal bar plot\",\n       subtitle = \"Using ggplot\")+\n  theme_bw()\n\n\n\n  ggsave(\"Figs/barplot.png\", bg = \"white\")\n\nAdição de faceta: Função facet_wrap\n\nsurvey |> \n  filter(state == \"RS\") |>\n  count(species, residue) |>\n  ggplot(aes(species, n))+\n  geom_col(width = 0.4,\n           fill = \"steelblue\")+\n  coord_flip()+\n  facet_wrap(~residue, ncol = 1)+\n  labs(x = \" \", y = \"Number of isolates\",\n       tiltle = \"Horizontal bar plot\",\n       subtitle = \"Using ggplot\")+\n  theme_bw()\n\n\n\n  ggsave(\"Figs/barplotfacet.png\", bg = \"white\")"
  },
  {
    "objectID": "Aula6.html",
    "href": "Aula6.html",
    "title": "Gráficos 2",
    "section": "",
    "text": "Um Scatterplot exibe a relação entre 2 variáveis numéricas, ajudando a identificar padrões ou correlações entre elas, cada ponto representa uma observação. Os gráficos de dispersão são frequentemente usados para representar dados que possuem duas variáveis contínuas, como comprimento e largura, tempo e velocidade, ou temperatura e umidade. A sua posição no eixo X (horizontal) e Y (vertical) representa os valores das 2 variáveis (primeiro aplica a variável x e depois a y). Usando ggplot2, os gráficos de dispersão são construídos usando a função geom_point. A função geom_point() compreende a seguinte estética:\n\nalpha: trabalha com a transparência dos pontos (pontos ficam mais transparentes e ajuda caso haja sobreposição);\ncolour: usada adicionar cor usa-se a (função color = nome da cor);\nfill: define a cor. O nome da cor, por ser texto, deve ser escrita entre aspas;\nshape: utilizada para adicionar diferentes formas ou tipos de marcadores para diferenciar as variáveis (ex. quadrado e triângulo);\nSize: define o tamanho do box, retorna um vetor numérico de comprimento length(x ).\n\nImportação dos dados: Para atribuir os dados de uma aba especifica do excel, usa-se o nome da aba e após isso a função read_excel seguido do nome do arquivo excel entre aspas (endereço) e logo após insere a vírgula e o nome da aba dos dados (também pode ser usado o número da aba de dados). O data frame deve ter o mesmo nome da aba quando for ser atribuido.\n\nlibrary(tidyverse)\nlibrary(readxl)\nfungicida_campo <- read_excel(\"dados-diversos.xlsx\", \n                              \"fungicida_campo\")\n\nVisualização: Depois, plota-se os dados definindo-se o eixo x e depois o eixo y. Usa-se o stat_summary para plotar a média de forma simples.\n\nfungicida_campo |> \n  ggplot(aes(trat, sev))+\n  geom_jitter(width = 0.1,\n              color = \"gray60\")+\n  stat_summary(fun = mean,\n               color = \"red\")\n\n\n\n\nPara plotar a média de forma rápida e não precisar do by_summurise = mean_se.\n\nfungicida_campo |> \n  ggplot(aes(trat, sev))+\n  geom_jitter(width = 0.1,\n              color = \"gray60\")+\n  stat_summary(fun.data = mean_se,\n               color = \"red\")\n\n\n\n\nMudando o eixo x para yld, um das variáveis do banco de dados fungicida_campo. Se inserir a função alpha dentro da função geom_point, os pontos ficam mais transparentes.\n\nlibrary(ggthemes)\nfungicida_campo |> \n  ggplot(aes(sev, yld,\n             color = trat))+\n  geom_point(size = 3)+\n  scale_color_colorblind()\n\n\n\n\nLinha de tendência: para criar uma linha de tendência, usa-se a função geom_smooth. Para retirar o erro, usa-se a função se = FALSE e linetype. Method lm ajusta os dados a função linear.\n\nfungicida_campo |> \n  ggplot(aes(sev, yld))+\n  geom_point(size = 3)+\nscale_color_colorblind()+\ngeom_smooth(method = \"lm\",\n            se = FALSE,\n            color = \"black\",\n            linetype = \"solid\",\n            size = 2)\n\n\n\n\n\n\nMudando o subconjunto de dados para milho: Ver a variação da produtividade dos hibridos de milho em função dos métodos de inoculação.\n\nmilho <- read_excel(\"dados-diversos.xlsx\", \"milho\")\nmilho |>\n  ggplot(aes(hybrid, yield, color = method))+\n  geom_jitter(size = 2)+\n  facet_wrap(~hybrid)\n\n\n\n#ou, para ver qual hibrido dá mais doença:\nmilho |>\n  ggplot(aes(method, index, color = method))+\n  geom_jitter(size = 2)+\n  facet_wrap(~hybrid)\n\n\n\n\n\n\n\n\nPara construir histogramas, usamos o geom_histogram. Esse geom só precisa do atributo x (o eixo y é construído automaticamente). Histogramas são úteis para avaliarmos a distribuição de uma variável. São frequentemente usados para representar dados contínuos, como medidas físicas, tempo, idade ou outras variáveis que podem ser divididas em intervalos. Os histogramas ajudam a visualizar a forma da distribuição de dados contínuos, incluindo informações sobre tendência central, variabilidade e assimetria. A função geom_histogram() suporta as seguintes estéticas:\n\nBins = refere-se ao número de barras presente no gráfico.\nColor = função usada para mudar a cor da linha do bloco.\nFill = função para mudar a cor dos blocos.\n\nAgora, montaremos um histograma brabalhando com 1 variável continua (produtividade). No histograma, tem-se os valores organizados em classe. Qual o padrão de distribuição dos pontos?\n\np_yield <- milho |>\n  ggplot(aes(x = yield))+\n  geom_histogram(bins = 10, color = \"black\", fill = \"green\")\n\nMudando de yield para index:\n\np_index <- milho |>\n  ggplot(aes(x = index))+\n  geom_histogram(bins = 10, color = \"black\", fill = \"blue\")\n\nCombinando gráficos com patchowork: deve-se primeiro definir um nome para cada gráfico\n\nlibrary(patchwork)\n(p_yield + p_index)+\nplot_annotation(tag_levels = \"A\")\n\n\n\nggsave(\"figs/histograms.png\", bg = \"white\")\n\n\n\n\nGráficos de densidade podem ajudar a identificar padrões, tendências, assimetrias e são geralmente usados para representar a distribuição de probabilidade de uma variável contínua (ex.:distribuição de alturas ou pesos de uma população, distribuição de pontuações em um teste padronizado). Para esse tipo de gráfico, a função a ser utilizada é geom_density().\n\nmilho |>\n  ggplot(aes(x = index))+\n  geom_density()\n\n\n\n\n\n\n\nTestando um novo subconjunto de dados: criou-se um gráfico de colunas, onde, no x tem-se inseticida e y é o status. Alterado de formato largo para longo.\n\ninsect <- read_excel(\"dados-diversos.xlsx\", \"mortalidade\")\ninsect |>\n  pivot_longer(2:3,\n               names_to = \"status\",\n               values_to = \"value\") |>\n  ggplot(aes(inseticida, value, fill = status))+\n  geom_col()"
  },
  {
    "objectID": "Aula7.html",
    "href": "Aula7.html",
    "title": "Aula7",
    "section": "",
    "text": "Antes de realizar uma análise estatística no R, pode ser necessário realizar transformações nos dados, dependendo da natureza dos dados e dos requisitos da análise. As transformações podem ajudar a atender aos pressupostos da análise estatística, como a normalidade e a homogeneidade de variâncias.\n\n\nAntes de realizar transformações, precisamos entender a natureza dos dados, então vamos nos familiarizar com um conjunto de dados, nesse caso, o conjunto de dados mofo, presente dentro do banco de dados da planilha dados-diversos.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nmofo <- read_excel(\"dados-diversos.xlsx\", \"mofo\")\n\nOs dados podem ser visualizados de diferentes formas - gráficos de dispersão, histogramas, boxplots, etc. Inicialmente, vamos visualizar os dados de inc (incidência) pelos tratamentos e pelo estudo (tratamento dentro de estudo). Então, o tratamento vai dentro do ggplot, enquanto o study vai dentro de facet. A função facet_wrap() replica o gráfico especificado para cada categoria de uma coluna.\n\nmofo |>\n  ggplot(aes(treat, inc))+\n  facet_wrap(~study)+\n  geom_point()\n\n\n\n\nTambém pode usar a função geom_col para visualizar os dados de uma forma diferente.\nHistograma: Faremos um histograma para visualizar a incidência e outro para visualizar os dados de escleródio.\n\ninc <- mofo |>\n  ggplot(aes(inc))+\n  geom_histogram()\n#Para o scleródio\nmofo |>\n    ggplot(aes(scl))+\n    geom_histogram()\n\n\n\n\nBoxplot: montaremos um boxplot para visualizar os dados de scl.\n\nscl <- mofo |>\n  ggplot(aes(scl))+\n  geom_boxplot()\n\nAgora, como já aprendido, juntaremos os 2 gráficos, para isso, devemos carregar o pacote patchwork:\n\nlibrary(patchwork)\ninc + scl\n\n\n\n\nApós isso, podemos encontrar a média dos dados. Para achar a média podemos usar as funções $, mean+conjunto ou summary.\n\nmofo$scl\n\n [1] 2194 1663 1313 1177  753 1343 1519  516  643  400  643  921 1196 1331  756\n[16]  338  581  588  231  925  119  394  206  275  131  588 5013 3619 2325 2588\n[31] 3969 1556 3175 1763 2894  350  419  644 2850 6216 2888 2272 2868 2412 2372\n[46] 3424 1744 1456 1732 1080 1592 3268\n\nmean(mofo$scl)\n\n[1] 1639.096\n\n\n\n\n\nNós podemos transformar os dados de diferentes maneiras, sendo as mais comuns log e raiz quadrada. Transformação logarítmica: é útil quando os dados possuem uma distribuição assimétrica positiva ou quando a variação aumenta exponencialmente com o aumento dos valores. Para transformar os dados para o logaritimo dos números usa-se a função log (). Podemos realizar essa transformação com o uso da função mutate. Através da função mutate() realizamos a criação/adição de uma nova variável (ou novas variaveis), que são funções de variáveis existentes, e também criamos/modificamos colunas.\n\nmofo2 <- mofo |>\n  mutate (scl2 = log(scl))\n  mofo2\n\n# A tibble: 52 × 6\n   study treat   inc   scl   yld  scl2\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1     1    76  2194  2265  7.69\n 2     1     2    53  1663  2618  7.42\n 3     1     3    42  1313  2554  7.18\n 4     1     4    37  1177  2632  7.07\n 5     1     5    29   753  2820  6.62\n 6     1     6    42  1343  2799  7.20\n 7     1     7    55  1519  2503  7.33\n 8     1     8    40   516  2967  6.25\n 9     1     9    26   643  2965  6.47\n10     1    10    18   400  3088  5.99\n# ℹ 42 more rows\n\n\nAgora, podemos visualizar os dados tranformados em log por meio de um histograma, da mesma forma feita acima.\n\nmofo2 |>\n    ggplot(aes(scl2))+\n    geom_histogram(bins = 10)\n\n\n\n\nTransformação em raiz quadrada: é útil para reduzir a assimetria em dados com uma distribuição assimétrica positiva. Para transformar os dados em raiz quadrada, usamos a função mutate e sqrt ().\n\nmofo2 <- mofo |>\n  mutate (scl2 = sqrt(scl))\n  mofo2\n\n# A tibble: 52 × 6\n   study treat   inc   scl   yld  scl2\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1     1    76  2194  2265  46.8\n 2     1     2    53  1663  2618  40.8\n 3     1     3    42  1313  2554  36.2\n 4     1     4    37  1177  2632  34.3\n 5     1     5    29   753  2820  27.4\n 6     1     6    42  1343  2799  36.6\n 7     1     7    55  1519  2503  39.0\n 8     1     8    40   516  2967  22.7\n 9     1     9    26   643  2965  25.4\n10     1    10    18   400  3088  20  \n# ℹ 42 more rows\n\n  mofo2 |>\n    ggplot(aes(scl2))+\n    geom_histogram(bins = 10)\n\n\n\n\nAgora, testaremos os dados da variável produtividade.\n\nmofo2 |>\n    ggplot(aes(yld))+\n    geom_histogram(bins = 10)\n\n\n\n\n\n\nFaremos uso agora da função arrange com o conjunto de dados survey. A função arrange () é usada para ordenar linhas. Essa função ordena os dados por ordem crescente ou decrescente (depende se coloca o sinal de menos ou não dentro da função). O primeiro argumento é a base de dados, os demais argumentos são as colunas pelas quais queremos ordenar as linhas. A função slice mostra determinada linha pela posição dela, que você solicita. A função filter escolhe casos com base em seus valores. A função count conta o número de linhas com cada valor exclusivo de uma variável (com ou sem pesos). A função slice seleciona linhas por posição.\n\nsurvey <- read_excel(\"dados-diversos.xlsx\", \"survey\")\n\n\nsurvey |>\n  filter(state == \"RS\") |>\n  count(species, residue) |>\n  arrange(n) |>\n#slice(1) |>\nrename(res = residue) |>\nmutate(n_class = case_when(\n  n < 30 ~ \"baixa\",\n       TRUE ~ \"Alta\"))\n\n# A tibble: 4 × 4\n  species res         n n_class\n  <chr>   <chr>   <int> <chr>  \n1 Fspp    corn       22 baixa  \n2 Fspp    soybean    26 baixa  \n3 Fgra    corn      147 Alta   \n4 Fgra    soybean   255 Alta"
  },
  {
    "objectID": "Aula8.html",
    "href": "Aula8.html",
    "title": "Teste estatístico",
    "section": "",
    "text": "Um teste de hipótese é uma técnica estatística cujo intuito é verificar se uma dada amostra de dados é, ou não, compatível com uma hipótese feita sobre a população que lhe deu origem. Um teste de hipóteses coloca lado-a-lado duas hipóteses sobre a população que deu origem à amostra de dados que temos à disposição - uma hipótese inicial, ou hipótese nula, e uma hipótese alternativa, normalmente designadas por H0 e H1 respetivamente e referem-se a uma caraterística de uma população. O objetivo do teste de hipótese é fornecer ferramentas que nos permitam validar ou rejeitar uma hipótese através dos resultados da amostra.\n\n\n\nHipótese de Nulidade (H0): é a hipótese a ser testada. Se testarmos, por exemplo, as igualdades entre duas médias, podem ter: H0: media1 (igual a) media2.\nHipótese Alternativa (Ha): é a hipótese que contraria H0. Para o caso das duas médias anteriores poderemos ter: Ha1: media1 (diferente da) media 2, Ha2: media1 < media2 e Ha3: media1 > media2.\n\n\n\n\nO p-valor é a probabilidade de observar o resultado encontrado (diferença) tão grande ou ainda maior (favorável a hipótese alternativa) considerando a hipotese nula como verdadeira. Em outras palavras, o p-valor (ou valor de probabilidade) indica a probabilidade de obter um resultado igual ou mais extremo do que o observado, assumindo-se que a hipótese nula (a hipótese de que não há diferença entre as amostras ou populações em estudo) seja verdadeira. O p-valor é uma medida de evidência contra a hipótese nula – um p-valor pequeno indica que é improvável que o resultado observado tenha ocorrido por acaso, o que sugere que a hipótese nula é falsa. Por outro lado, um p-valor grande indica que é provável que o resultado observado tenha ocorrido por acaso, o que sugere que a hipótese nula é verdadeira. O valor do p-valor é geralmente comparado com um nível de significância pré-determinado (geralmente 0,05 ou 0,01) para decidir se rejeita-se ou não a hipótese nula. Se o p-valor for menor do que o nível de significância, rejeita-se a hipótese nula e conclui-se que há evidência estatística de que há diferença entre as amostras ou populações em estudo. Caso contrário, não se rejeita a hipótese nula e conclui-se que não há evidência estatística de que haja diferença entre as amostras ou populações em estudo.\nExportação e visualização dos dados: usando os dados do conjunto magnésio, plotamos um gráfico boxplot. Usamos a função annotate() para escrever anotações dentro do gráfico.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nmg <- read_excel(\"dados-diversos.xlsx\")\nmg |>\n  ggplot(aes(trat, comp))+\n  geom_jitter(width = 0.05)+\n  geom_boxplot(fill = NA,\n               outlier.colour = NA)+\n  ylim(5, 20)+\n  annotate(geom = \"text\",\n           x = 0.7, y = 19,\n           label = \"t = 8.15; p < 0.001\")\n\n\n\n\n\n\n\nAbaixo, criamos um novo objeto (mg2) e atribuímos mg a ele para realizar a função pivot_wider(). Esta função “alarga” os dados, aumentando o número de colunas e diminuindo o número de linhas. Nesta função usa-se names_from e values_from, onde: names_from refere-se a coluna cujos valores serão usados como nomes de coluna e values_from refere-se a coluna cujos valores serão usados como valores de célula.\n\nmg2 <- mg |> \n  pivot_wider(1,\n              names_from = trat,\n              values_from = comp)\n\nFunção t.test: usa-se a função t test() para calcular o valor de t. Primeiro, realizamos um teste t para comparar as médias dos grupos “Mg2” e “control” no data frame “mg2”. Após isso, o pacote “report” é carregado e a função report() é usada para gerar um relatório formatado dos resultados do teste t.\n\nt <- t.test(mg2$Mg2, mg2$control)\n\nlibrary(report)\nreport(t)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference between mg2$Mg2 and\nmg2$control (mean of x = 10.52, mean of y = 15.68) suggests that the effect is\nnegative, statistically significant, and large (difference = -5.16, 95% CI\n[-6.49, -3.83], t(17.35) = -8.15, p < .001; Cohen's d = -3.65, 95% CI [-5.12,\n-2.14])\n\n\nVisualizando o resultado: agora, plotamos um gráfico simples usando o data frame “mg” e as variáveis trat e comp como os eixos x e y. Em seguida, solicitamos um resumo estatístico completo (função stat_summary com fun.data) ao gráfico mostrando as médias e os erros padrão da média para cada nível da variável trat (mean_se).\nFun.data: função de resumo completa - uma função que recebe os dados completos e deve retornar um quadro de dados com variáveis ymin, y e ymax. Deve receber o quadro de dados como entrada e retornar o quadro de dados como saída.\n\nmg |>\n  ggplot(aes(trat, comp))+\n  stat_summary(fun.data = \"mean_se\")\n\n\n\n\n\n\n\nO pacote infer realiza a inferência estatística usando uma linguagem que seja coerente com a estrutura do tidyverse. O pacote é centrado em 4 funções principais para visualizar e extrair valores: specify() - permite especificar a variável, ou relacionamento entre variáveis, em que se está interessado. hypothesize() - permite declarar a hipótese nula. generate() - permite gerar dados que refletem a hipótese nula. calculate() - permite calcular uma distribuição de estatísticas a partir dos dados gerados para formar a distribuição nula.\n\nlibrary(infer) \nmg |> t_test(comp ~ trat, order =\nc(\"control\", \"Mg2\"))\n\n# A tibble: 1 × 7\n  statistic  t_df     p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>       <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1      8.15  17.4 0.000000242 two.sided       5.16     3.83     6.49\n\n\nAcima, pedimos que o pacote infer fosse carregado e, em seguida, realizamos um teste t usando a função t_test() para comparar as médias dos grupos definidos pela variável “trat” no data frame “mg2”. A ordem dos níveis da variável “trat” é especificada para garantir que a comparação seja feita corretamente."
  },
  {
    "objectID": "Aula9.html",
    "href": "Aula9.html",
    "title": "Análises estatísticas",
    "section": "",
    "text": "Veremos nesta aula os tipos de análise de acordo com o número e tipo de variáveis independentes (níveis do fator) e também o número de tratamentos ou grupos a serem comparados.\n\n\nO teste t compara DUAS MÉDIAS (2 tratamentos) e mostra se as diferenças entre essas médias são significativas. Como todo teste estatístico, o teste t também tem como produto a medida do valor de p. Ou seja, no final das contas, teremos calculado a probabilidade da diferença encontrada (entre as médias) terem sido por acaso. Existem 2 tipos mais comuns de teste t:\n\nteste t para 2 amostras dependentes (ou pareadas): compara as médias da mesma população em diferentes momentos de tempo (ex.: antes e depois).\nteste t para 2 amostras independentes (ou não pareadas): compara as médias de duas populações distintas.\n\nOs termos paramétrico e não-paramétrico referem-se à média e ao desvio-padrão, que são os parâmetros que definem as populações que apresentam distribuição normal. Veremos como trabalahar nessas duas diferentes sutuações.\n\n\nSituação:Um pesquisador conduziu um experimento com o objetivo de avaliar o efeito de um micronutriente, o magnésio (Mg), adicionado na solução do solo cultivado com plantas de arroz, no manejo de uma doença fúngica. O experimento foi conduzido em delineamento inteiramente casualizado com 10 repetições, sendo cada repetição um vaso de planta. Um dos tratamentos é o chamado controle, ou testemunha, sem o suplemento mineral. O segundo é aquele com o suplemento do Mg na dose de 2 mM. Em cada uma das repetições foi obtido um valor médio do comprimento de lesões em um determinado tempo após a inoculação.\nPreparo pré-análise: carregamento de pacotes e importação do conjunto de dados.\n\nlibrary(magrittr) # para usar pipes\nlibrary(ggplot2) # para gráficos\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(tidyr)\n\n\ndata_mg <- read_excel(\"dados-diversos.xlsx\")\nhead(data_mg)\n\n# A tibble: 6 × 3\n  trat    rep  comp\n  <chr> <dbl> <dbl>\n1 Mg2       1   9  \n2 Mg2       2  12.5\n3 Mg2       3  10  \n4 Mg2       4   8  \n5 Mg2       5  13.2\n6 Mg2       6  11  \n\n\nAgora, vamos começar a trabalhar esses dados e obter estatísticas que descrevem o conjunto de dados, seja a tendência central ou a dispersão dos dados. Neste caso, trabalhamos com a média (mean), variância (var), desvio padrão (sd), erro padrão (se) e intervalo de confiança (ci). O intervalo de confiança é apenas para inferência visual.\n\ndata2 <- data_mg %>%\n  group_by(trat) %>%\n  summarise(\n    mean_com = mean(comp),\n    sd_comp = sd(comp),\n    var_comp = var(comp),\n    n = n(),\n    se_comp = sd_comp / sqrt(n - 1),\n    ci = se_comp * qt(0,025, df = 9))\ndata2\n\n# A tibble: 2 × 7\n  trat    mean_com sd_comp var_comp     n se_comp    ci\n  <chr>      <dbl>   <dbl>    <dbl> <int>   <dbl> <dbl>\n1 Mg2         10.5    1.54     2.39    10   0.515  -Inf\n2 control     15.7    1.27     1.61    10   0.424  -Inf\n\n\nVisualização: A maneira mais simples é visualizar, no caso de mais de 6 repetições, usando boxplots juntamente com os dados de cada repetição. Aqui visualizaremos os dados em gráfico de barras vertical com erro padrão.\n\ndata2 |> \n  ggplot(aes(trat, mean_com)) +\n  geom_col(width = 0.5,\n           fill = \"steelblue\") +\n  geom_errorbar(aes(\n    ymin = mean_com - se_comp,\n    ymax = mean_com + se_comp),\n    width = 0.1) +\n  ylim(0,20) +\nlabs(x = \"\", y = \"Mean size (mm)\")\n\n\n\n\nIntervalo de confiança: Agora visualizamos os dados com o ci. Abaixo, as barras verticais representam o intervalo de confiança 95%.\n\ndata2 |> \n  ggplot(aes(trat, mean_com)) +\n  geom_col(width = 0.5, fill = \"steelblue\") +\n  geom_errorbar(aes(\n    ymin = mean_com - ci,\n    ymax = mean_com + ci),\n    width = 0.1) +\n  ylim(0,20) +\nlabs(x = \"\", y = \"Mean size (mm)\")\n\n\n\n\nO conjunto de dados está no formato largo, assim a variável resposta de interesse está apenas em uma coluna. Existem várias formas de separar em dois vetores os dados de resposta para cada tratamento. Uma delas é por meio da função pivot_winder, a qual coloca as respostas em duas colunas, uma para cada tratamento. Para isso, criaremos o conjunto data_mg2. Agora é posspivel visualizar as respostas (tamanho da lesão) para cada tratamento usando o conjunto de dados original, já que O ggplot2 requer os dados no formato largo.\n\ndata_mg2 <- data_mg |>\n  pivot_wider(1,\n              names_from = trat,\n              values_from = comp)\ndata_mg2\n\n# A tibble: 10 × 3\n     rep   Mg2 control\n   <dbl> <dbl>   <dbl>\n 1     1   9      13.7\n 2     2  12.5    15.9\n 3     3  10      15.7\n 4     4   8      14.2\n 5     5  13.2    15.9\n 6     6  11      16.5\n 7     7  10.8    18  \n 8     8   9.5    14.4\n 9     9  10.8    16.4\n10    10  10.4    16  \n\n\n\n\nTeste t é um teste paramétrico e para ele precisa seguir 2 premissas - normalidade e homogeneidade da variância (homocedasticidade). Caso hja necessidade de transformação dos dados para deixá-los normais, é aceitável.\n\nt.test(data_mg2$Mg2, data_mg2$control, \npaired = F)\n\n\n    Welch Two Sample t-test\n\ndata:  data_mg2$Mg2 and data_mg2$control\nt = -8.1549, df = 17.354, p-value = 2.423e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.490393 -3.825607\nsample estimates:\nmean of x mean of y \n   10.520    15.678 \n\n\nA ordem de execução dos testes deve ser: Shapiro.teste (teste de normalidade) > var.test (teste de homocedasticidade) > se p menor que 0,05 > t.test. No caso da variância dar heterogênea, var EQUAL = F. No caso dos dados pareados usa-se o argumento pared = TRUE\n\n\nPodemos confirmar a premissa da homocedasticidade pelo teste F. No caso de dois grupos, a função que pode ser usada é a var.test do R. Vamos usar o formato largo e chamar os dois vetores do conjunto. Verifique o P-valor na saída da análise.\n\nattach(data_mg2)\nvar.test(Mg2, control)\n\n\n    F test to compare two variances\n\ndata:  Mg2 and control\nF = 1.4781, num df = 9, denom df = 9, p-value = 0.5698\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3671417 5.9508644\nsample estimates:\nratio of variances \n          1.478111 \n\n\nA verificação deste pressuposto também pode ser realizada graficamente através do boxplot para os tratamentos vs resíduos. Se existir homocedasticidade espera-se que os boxplots sejam semelhantes.\n\n\n\nA normalidade pode ser testada por meio de procedimentos visuais e testes específicos. Para testar a normalidade dos dados, fazemos o teste shapiro.\n\nshapiro.test(Mg2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Mg2\nW = 0.97269, p-value = 0.9146\n\nshapiro.test(control)\n\n\n    Shapiro-Wilk normality test\n\ndata:  control\nW = 0.93886, p-value = 0.5404\n\n\nAnálise visual da premissa de normalidade: A análise visual da premissa de normalidade é realizada por qqplot (QQ-Plot), que permite verificar se uma amostra segue uma distribuição gaussiana. Podemos simplesmente fazer usando as funções qqnorm() e qqline() para cada umas das variáveis analisadas.\n\nqqnorm (Mg2)\nqqline(Mg2)\n\n\n\nqqnorm(control)\nqqline(control)\n\n\n\n\n\n\n\n\n\nSe as premissas de normalidade não fossem atendidas, qual o teste que poderia ser usado? Nesse caso de dois grupos há duas possibilidades, uma é usar um teste não paramétrico ou um teste baseado em reamostragem (bootstrapping) dos dados, os quais independem do modelo de distribuição. Vejamos um exemplo.\nSituação: Um experimento foi conduzido para avaliar o efeito do uso da escala na acurácia e precisão de avaliações visuais de severidade por avaliadores. A hipótese a ser testada foi que avaliações utilizando uma escala digramática como auxílio são mais acuradas do que sem o uso do auxílio. Dez avaliadores foram escolhidos aleatoriamente e fizeram duas avaliações cada. Cinco variáveis que compõe a medida da concordância das estimativas foram obtidas. Uma vez que as medidas foram repetidas no tempo para cada avaliador, as amostras são do tipo dependentes.\nPreparo pré-análise: importação dos dados e preparo do conjunto.\n\nescala <- read_excel(\"dados-diversos.xlsx\", \"escala\")\nhead(escala)\n\n# A tibble: 6 × 7\n  assessment rater acuracia precisao vies_geral vies_sistematico vies_constante\n  <chr>      <chr>    <dbl>    <dbl>      <dbl>            <dbl>          <dbl>\n1 Unaided    A        0.809    0.826      0.979            1.19         0.112  \n2 Unaided    B        0.722    0.728      0.991            0.922       -0.106  \n3 Unaided    C        0.560    0.715      0.783            1.16         0.730  \n4 Unaided    D        0.818    0.819      0.999            0.948       -0.00569\n5 Unaided    E        0.748    0.753      0.993            1.10         0.0719 \n6 Unaided    F        0.695    0.751      0.925            0.802        0.336  \n\nescala2 <- escala |> \n  select(assessment, rater, acuracia)\nescala3 <- escala2|>\n  pivot_wider(1,\n              names_from = assessment,\n              values_from = acuracia)\n\nChecagem das premissas:\n\n## homocedasticidade dois grupos\nattach(escala3)\nvar.test(Aided1, Unaided)\n\n\n    F test to compare two variances\n\ndata:  Aided1 and Unaided\nF = 0.17041, num df = 9, denom df = 9, p-value = 0.01461\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.04232677 0.68605885\nsample estimates:\nratio of variances \n         0.1704073 \n\n## normalidade\nshapiro.test(Aided1)$p.value\n\n[1] 0.4260888\n\nshapiro.test(Unaided)$p.value\n\n[1] 0.1131276\n\n\nAnálise visual da normalidade:\n\nqqnorm(Aided1)\nqqline(Aided1)\n\n\n\nqqnorm(Unaided)\nqqline(Unaided)\n\n\n\n\n\n\n\n\n## teste t para amostras pareadas\nt_escala <- t.test(escala3$Aided1, escala3$Unaided,\n  paired = TRUE,\n  var.equal = F\n)\n\nt_escala\n\n\n    Paired t-test\n\ndata:  escala3$Aided1 and escala3$Unaided\nt = 5.9364, df = 9, p-value = 0.000219\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.1144707 0.2554241\nsample estimates:\nmean difference \n      0.1849474 \n\n\n\n\n\nUm teste não paramétrico não faz nenhuma suposição sobre a distribuição da população ou tamanho da amostra. O Wilcox.test é o teste para dados não paramétricos equivalente ao teste t para dados paramétricos. o teste de Wilcoxon é usado para testar se as medianas das amostras são iguais nos casos em que a suposição de normalidade não é satisfeita ou quando não for possível checar essa suposição.\n\nwilcox.test(escala3$Aided1, escala3$Unaided, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  escala3$Aided1 and escala3$Unaided\nV = 55, p-value = 0.001953\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FIP606",
    "section": "",
    "text": "Instagram\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Githube\n  \n  \n    \n     Email\n  \n\n\n\n\nBem-vindo ao meu website!\nEu sou Nívia Maria, doutoranda do programa de pós-graduação em Fitopatologia da Universidade Federal de Viçosa. Eu possuo graduação em Engenharia Agronômica pela Universidade Federal de Sergipe (Aracaju, SE) e mestrado em Fitopatologia (2022) pela Universidade Federal de Viçosa.\nEste website foi criado por mim em Quarto Markdown para a disciplina FIP606 - Análise e visualização de dados em Fitopatologia, ministrada pelo professor Emerson Del Ponte. Neste site, você encontrará o material das aulas ministradas organizados em tópicos. Convido você a explorar meu site e um pouco sobre o fascinante mundo da análise de dados fitopatológicos no Rstudio.\nSinta-se à vontade para entrar em contato comigo se tiver alguma dúvida."
  }
]